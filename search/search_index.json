{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>Python \ub370\uc774\ud130 \ubd84\uc11d &amp; Machine Learning \uac15\uc758\uc5d0 \uc624\uc2e0 \uac83\uc744 \ud658\uc601\ud569\ub2c8\ub2e4.  \uc88c\uce21 \ub124\ube44\uac8c\uc774\uc158\uc744 \ub530\ub77c \uac00\uc2dc\uba74 \ub429\ub2c8\ub2e4.  </p>"},{"location":"#contents","title":"Contents","text":""},{"location":"#_1","title":"\ud30c\uc774\uc36c \ub370\uc774\ud130 \ubd84\uc11d","text":"<ul> <li>Python \uae30\ucd08</li> <li>Data \ubc1b\uc544\uc624\uae30 (Library \uc0ac\uc6a9)</li> <li>pandas</li> <li>Visualization</li> </ul>"},{"location":"#_2","title":"\uba38\uc2e0 \ub7ec\ub2dd","text":"<ul> <li>\ud68c\uadc0 (Regression)</li> <li>\ubd84\ub958 (Classification)</li> <li>\uacfc\uc801\ud569 (Overfit), \ud558\uc774\ud37c \ud30c\ub77c\ubbf8\ud130 (Hyper Param)</li> </ul>"},{"location":"machine_learning_tutorial/01_Regression/","title":"Machine Learning","text":"In\u00a0[14]: Copied! <pre>import pandas as pd \n\ncolumn_names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV']\n\ndf = pd.read_csv(\"../../data/boston_housing.csv\",  header=None, delimiter=r\"\\s+\", names=column_names)\ndf\n</pre> import pandas as pd   column_names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV']  df = pd.read_csv(\"../../data/boston_housing.csv\",  header=None, delimiter=r\"\\s+\", names=column_names) df Out[14]: CRIM ZN INDUS CHAS NOX RM AGE DIS RAD TAX PTRATIO B LSTAT MEDV 0 0.00632 18.0 2.31 0 0.538 6.575 65.2 4.0900 1 296.0 15.3 396.90 4.98 24.0 1 0.02731 0.0 7.07 0 0.469 6.421 78.9 4.9671 2 242.0 17.8 396.90 9.14 21.6 2 0.02729 0.0 7.07 0 0.469 7.185 61.1 4.9671 2 242.0 17.8 392.83 4.03 34.7 3 0.03237 0.0 2.18 0 0.458 6.998 45.8 6.0622 3 222.0 18.7 394.63 2.94 33.4 4 0.06905 0.0 2.18 0 0.458 7.147 54.2 6.0622 3 222.0 18.7 396.90 5.33 36.2 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 501 0.06263 0.0 11.93 0 0.573 6.593 69.1 2.4786 1 273.0 21.0 391.99 9.67 22.4 502 0.04527 0.0 11.93 0 0.573 6.120 76.7 2.2875 1 273.0 21.0 396.90 9.08 20.6 503 0.06076 0.0 11.93 0 0.573 6.976 91.0 2.1675 1 273.0 21.0 396.90 5.64 23.9 504 0.10959 0.0 11.93 0 0.573 6.794 89.3 2.3889 1 273.0 21.0 393.45 6.48 22.0 505 0.04741 0.0 11.93 0 0.573 6.030 80.8 2.5050 1 273.0 21.0 396.90 7.88 11.9 <p>506 rows \u00d7 14 columns</p> In\u00a0[16]: Copied! <pre># X / Y Split\nY = df['MEDV']\nX = df.drop('MEDV', axis=1)\nX\n</pre> # X / Y Split Y = df['MEDV'] X = df.drop('MEDV', axis=1) X Out[16]: CRIM ZN INDUS CHAS NOX RM AGE DIS RAD TAX PTRATIO B LSTAT 0 0.00632 18.0 2.31 0 0.538 6.575 65.2 4.0900 1 296.0 15.3 396.90 4.98 1 0.02731 0.0 7.07 0 0.469 6.421 78.9 4.9671 2 242.0 17.8 396.90 9.14 2 0.02729 0.0 7.07 0 0.469 7.185 61.1 4.9671 2 242.0 17.8 392.83 4.03 3 0.03237 0.0 2.18 0 0.458 6.998 45.8 6.0622 3 222.0 18.7 394.63 2.94 4 0.06905 0.0 2.18 0 0.458 7.147 54.2 6.0622 3 222.0 18.7 396.90 5.33 ... ... ... ... ... ... ... ... ... ... ... ... ... ... 501 0.06263 0.0 11.93 0 0.573 6.593 69.1 2.4786 1 273.0 21.0 391.99 9.67 502 0.04527 0.0 11.93 0 0.573 6.120 76.7 2.2875 1 273.0 21.0 396.90 9.08 503 0.06076 0.0 11.93 0 0.573 6.976 91.0 2.1675 1 273.0 21.0 396.90 5.64 504 0.10959 0.0 11.93 0 0.573 6.794 89.3 2.3889 1 273.0 21.0 393.45 6.48 505 0.04741 0.0 11.93 0 0.573 6.030 80.8 2.5050 1 273.0 21.0 396.90 7.88 <p>506 rows \u00d7 13 columns</p> In\u00a0[8]: Copied! <pre>%pip install -U scikit-learn\n</pre> %pip install -U scikit-learn <pre>Collecting scikit-learn\n  Downloading scikit_learn-1.6.0-cp312-cp312-macosx_12_0_arm64.whl.metadata (31 kB)\nRequirement already satisfied: numpy&gt;=1.19.5 in /Users/jonhpark/workspace/courses_archive/mkdocs_venv/lib/python3.12/site-packages (from scikit-learn) (2.0.1)\nCollecting scipy&gt;=1.6.0 (from scikit-learn)\n  Downloading scipy-1.14.1-cp312-cp312-macosx_14_0_arm64.whl.metadata (60 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 60.8/60.8 kB 4.5 MB/s eta 0:00:00\nCollecting joblib&gt;=1.2.0 (from scikit-learn)\n  Using cached joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\nCollecting threadpoolctl&gt;=3.1.0 (from scikit-learn)\n  Using cached threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\nDownloading scikit_learn-1.6.0-cp312-cp312-macosx_12_0_arm64.whl (11.2 MB)\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 11.2/11.2 MB 19.6 MB/s eta 0:00:0000:0100:01\nUsing cached joblib-1.4.2-py3-none-any.whl (301 kB)\nDownloading scipy-1.14.1-cp312-cp312-macosx_14_0_arm64.whl (23.1 MB)\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 23.1/23.1 MB 22.8 MB/s eta 0:00:0000:0100:01\nUsing cached threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\nInstalling collected packages: threadpoolctl, scipy, joblib, scikit-learn\nSuccessfully installed joblib-1.4.2 scikit-learn-1.6.0 scipy-1.14.1 threadpoolctl-3.5.0\n\n[notice] A new release of pip is available: 24.0 -&gt; 24.3.1\n[notice] To update, run: pip install --upgrade pip\nNote: you may need to restart the kernel to use updated packages.\n</pre> In\u00a0[17]: Copied! <pre># \ub370\uc774\ud130\uc14b \ubd84\ub9ac\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_valid, Y_train, Y_valid = train_test_split(X,Y)\n</pre> # \ub370\uc774\ud130\uc14b \ubd84\ub9ac from sklearn.model_selection import train_test_split  X_train, X_valid, Y_train, Y_valid = train_test_split(X,Y) In\u00a0[18]: Copied! <pre># \ubaa8\ub378 \uc815\uc758\nfrom sklearn.linear_model import LinearRegression\n\nmodel = LinearRegression()\n</pre> # \ubaa8\ub378 \uc815\uc758 from sklearn.linear_model import LinearRegression  model = LinearRegression() In\u00a0[19]: Copied! <pre># \ubaa8\ub378 \ud559\uc2b5\nmodel.fit(X_train, Y_train)\n</pre> # \ubaa8\ub378 \ud559\uc2b5 model.fit(X_train, Y_train) Out[19]: <pre>LinearRegression()</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegression?Documentation for LinearRegressioniFitted<pre>LinearRegression()</pre> In\u00a0[20]: Copied! <pre>from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score, mean_squared_error\n\n\n# Predict on the validation set\nY_pred = model.predict(X_valid)\n\n# Calculate scores\nr2 = r2_score(Y_valid, Y_pred)\nmse = mean_squared_error(Y_valid, Y_pred)\nprint(f\"R^2 Score: {r2:.2f}\")\nprint(f\"Mean Squared Error: {mse:.2f}\")\n</pre> from sklearn.model_selection import train_test_split from sklearn.linear_model import LinearRegression from sklearn.metrics import r2_score, mean_squared_error   # Predict on the validation set Y_pred = model.predict(X_valid)  # Calculate scores r2 = r2_score(Y_valid, Y_pred) mse = mean_squared_error(Y_valid, Y_pred) print(f\"R^2 Score: {r2:.2f}\") print(f\"Mean Squared Error: {mse:.2f}\") <pre>R^2 Score: 0.72\nMean Squared Error: 25.41\n</pre> In\u00a0[22]: Copied! <pre>import plotly.express as px\n\n# Convert X_valid to DataFrame for easier hover display\nif hasattr(X_valid, \"toarray\"):  # Handle sparse matrices\n    X_valid_df = pd.DataFrame(X_valid.toarray())\nelse:\n    X_valid_df = pd.DataFrame(X_valid)\n\n# Prepare data for plotting\nscatter_data = pd.DataFrame({\n    \"Actual\": Y_valid,\n    \"Predicted\": Y_pred,\n    \"X Values\": X_valid_df.values.tolist()  # List of X values for hover\n})\n\n# Create scatter plot with Plotly\nfig = px.scatter(\n    scatter_data,\n    x=\"Actual\",\n    y=\"Predicted\",\n    hover_data={\"X Values\": True},  # Show X values in hover\n    labels={\"x\": \"Actual Values\", \"y\": \"Predicted Values\"},\n    title=\"Actual vs Predicted with X Values\"\n)\nfig.add_shape(\n    type=\"line\",\n    x0=scatter_data[\"Actual\"].min(),\n    y0=scatter_data[\"Actual\"].min(),\n    x1=scatter_data[\"Actual\"].max(),\n    y1=scatter_data[\"Actual\"].max(),\n    line=dict(color=\"Red\", dash=\"dash\"),\n    name=\"Perfect Prediction\"\n)\n\nfig.show()\n</pre> import plotly.express as px  # Convert X_valid to DataFrame for easier hover display if hasattr(X_valid, \"toarray\"):  # Handle sparse matrices     X_valid_df = pd.DataFrame(X_valid.toarray()) else:     X_valid_df = pd.DataFrame(X_valid)  # Prepare data for plotting scatter_data = pd.DataFrame({     \"Actual\": Y_valid,     \"Predicted\": Y_pred,     \"X Values\": X_valid_df.values.tolist()  # List of X values for hover })  # Create scatter plot with Plotly fig = px.scatter(     scatter_data,     x=\"Actual\",     y=\"Predicted\",     hover_data={\"X Values\": True},  # Show X values in hover     labels={\"x\": \"Actual Values\", \"y\": \"Predicted Values\"},     title=\"Actual vs Predicted with X Values\" ) fig.add_shape(     type=\"line\",     x0=scatter_data[\"Actual\"].min(),     y0=scatter_data[\"Actual\"].min(),     x1=scatter_data[\"Actual\"].max(),     y1=scatter_data[\"Actual\"].max(),     line=dict(color=\"Red\", dash=\"dash\"),     name=\"Perfect Prediction\" )  fig.show() In\u00a0[23]: Copied! <pre># Get feature importance (coefficients)\nfeature_importance = model.coef_\n\n# If X is a DataFrame, get the column names, else use generic names\nif isinstance(X, pd.DataFrame):\n    feature_names = X.columns\nelse:\n    feature_names = [f\"Feature {i}\" for i in range(X.shape[1])]\n\n# Create a DataFrame for feature importance\nimportance_df = pd.DataFrame({\n    \"Feature\": feature_names,\n    \"Importance\": feature_importance\n}).sort_values(by=\"Importance\", ascending=False)\n\n# Print feature importance\nprint(importance_df)\n\n# Plot feature importance\nfig = px.bar(\n    importance_df,\n    x=\"Feature\",\n    y=\"Importance\",\n    title=\"Feature Importance\",\n    labels={\"Importance\": \"Coefficient\"},\n    text=\"Importance\"\n)\nfig.update_traces(texttemplate='%{text:.2f}', textposition='outside')\nfig.show()\n</pre> # Get feature importance (coefficients) feature_importance = model.coef_  # If X is a DataFrame, get the column names, else use generic names if isinstance(X, pd.DataFrame):     feature_names = X.columns else:     feature_names = [f\"Feature {i}\" for i in range(X.shape[1])]  # Create a DataFrame for feature importance importance_df = pd.DataFrame({     \"Feature\": feature_names,     \"Importance\": feature_importance }).sort_values(by=\"Importance\", ascending=False)  # Print feature importance print(importance_df)  # Plot feature importance fig = px.bar(     importance_df,     x=\"Feature\",     y=\"Importance\",     title=\"Feature Importance\",     labels={\"Importance\": \"Coefficient\"},     text=\"Importance\" ) fig.update_traces(texttemplate='%{text:.2f}', textposition='outside') fig.show() <pre>    Feature  Importance\n5        RM    3.641342\n3      CHAS    1.807958\n8       RAD    0.347263\n2     INDUS    0.073697\n1        ZN    0.058716\n6       AGE    0.008889\n11        B    0.008377\n9       TAX   -0.015813\n0      CRIM   -0.102134\n12    LSTAT   -0.518889\n10  PTRATIO   -1.005465\n7       DIS   -1.523085\n4       NOX  -20.377928\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"machine_learning_tutorial/01_Regression/#machine-learning","title":"Machine Learning\u00b6","text":""},{"location":"machine_learning_tutorial/01_Regression/#1","title":"1. \ucee8\uc149\u00b6","text":""},{"location":"machine_learning_tutorial/01_Regression/#y-fx","title":"$$y = f(x)$$\u00b6","text":"<ul> <li>$y$ = \ud574\uacb0\ud574\uc57c \ud558\ub294 \ubb38\uc81c (\uc885\uc18d\ubcc0\uc218, target data)</li> <li>$f()$ = \uba38\uc2e0\ub7ec\ub2dd \uc54c\uace0\ub9ac\uc998 (\ubaa8\ub378)</li> <li>$x$ = \ud6c8\ub828\ub370\uc774\ud130 (\uc124\uba85\ubcc0\uc218, train data)</li> </ul> <p>x -&gt; y \ub97c \uc798 \ub9de\ucd94\ub294 \ubaa8\ub378 (\ud568\uc218) \ub97c \ub9cc\ub4dc\ub294 \uac83\uc774 \ubaa9\ud45c\uc774\uace0, \uc774\ub97c \ub370\uc774\ud130\ub97c \uae30\ubc18\uc73c\ub85c \ud559\uc2b5\ud574\uc11c \ub9cc\ub4e4\uaca0\ub2e4\ub294 \uac83\uc774 \uba38\uc2e0 \ub7ec\ub2dd \uc785\ub2c8\ub2e4.</p>"},{"location":"machine_learning_tutorial/01_Regression/#2","title":"2. \uc885\ub958\u00b6","text":""},{"location":"machine_learning_tutorial/01_Regression/#21-regression-prediction","title":"2.1. \ud68c\uadc0 (Regression) \ub610\ub294 \uc608\uce21 (Prediction)\u00b6","text":"<ul> <li>output y \uac00 \uc5f0\uc18d\uc801\uc778 value \uc778 \uacbd\uc6b0</li> <li>ex. \uc8fc\uac00 \uc608\uce21</li> </ul>"},{"location":"machine_learning_tutorial/01_Regression/#22-classification","title":"2.2. \ubd84\ub958 (Classification)\u00b6","text":"<ul> <li>output y \uac00 \ube44\uc5f0\uc18d\uc801\uc778 \uacbd\uc6b0 (\uce74\ud14c\uace0\ub9ac \ud0c0\uc785)</li> <li>ex. \uc81c\ud488\uc774 \ubd88\ub7c9\uc778\uc9c0 \uc591\ud488\uc778\uc9c0 \ubd84\ub958\ud558\uae30</li> </ul>"},{"location":"machine_learning_tutorial/01_Regression/#23-clustering","title":"2.3. \uad70\uc9d1\ud654 (Clustering)\u00b6","text":"<ul> <li>output y \uac00 \ub611\uac19\uc774 \uce74\ud14c\uace0\ub9ac\uc774\ub098, \uc815\ub2f5 \uc5c6\uc774 \ud559\uc2b5\ud568</li> <li>\ube44\uc9c0\ub3c4 \ud559\uc2b5 (Unsupervised Learning)</li> </ul>"},{"location":"machine_learning_tutorial/01_Regression/#3-machine-learning","title":"3. Machine Learning \uc608\uc2dc\u00b6","text":"<ul> <li>\uc5ec\ub7ec \uc815\ubcf4\ub97c \uc774\uc6a9\ud574\uc11c \uc8fc\ud0dd \uac00\uaca9\uc744 \ub9de\ucdb0\ubcf4\uc790!</li> <li>data \ud3f4\ub354\uc758 boston_housing.csv \ub85c \uc608\uc2dc</li> </ul> <p>\ucc38\uc870 - \uce90\uae00\uc758 Boston \uc9d1\uac12 \ub370\uc774\ud130 \uc5f4\ub78c</p>"},{"location":"machine_learning_tutorial/01_Regression/#31-dataset","title":"3.1. dataset \ub85c\ub4dc\ud558\uae30\u00b6","text":"<p>\uc804\uccb4 (\ud559\uc2b5) \ub370\uc774\ud130\ub97c \ub85c\ub4dc\ud569\ub2c8\ub2e4.</p> <p></p>"},{"location":"machine_learning_tutorial/01_Regression/#32-x-y","title":"3.2. X, Y \uc815\uc758 \ud558\uae30\u00b6","text":""},{"location":"machine_learning_tutorial/01_Regression/#33-train-text-valid","title":"3.3. Train / Text (Valid) \uc2a4\ud50c\ub9bf\u00b6","text":""},{"location":"machine_learning_tutorial/01_Regression/#34-model","title":"3.4. Model \uc815\uc758\ud558\uae30\u00b6","text":""},{"location":"machine_learning_tutorial/01_Regression/#35-model","title":"3.5. Model \ud559\uc2b5\ud558\uae30\u00b6","text":""},{"location":"machine_learning_tutorial/01_Regression/#36","title":"3.6. \ubaa8\ub378 \ud3c9\uac00\ud558\uae30\u00b6","text":""},{"location":"machine_learning_tutorial/02_classification/","title":"Machine Learning - Classification","text":"In\u00a0[1]: Copied! <pre>import pandas as pd \n\ndf = pd.read_csv(\"../../data/WineQT.csv\")\ndf\n</pre> import pandas as pd   df = pd.read_csv(\"../../data/WineQT.csv\") df Out[1]: fixed acidity volatile acidity citric acid residual sugar chlorides free sulfur dioxide total sulfur dioxide density pH sulphates alcohol quality Id 0 7.4 0.700 0.00 1.9 0.076 11.0 34.0 0.99780 3.51 0.56 9.4 5 0 1 7.8 0.880 0.00 2.6 0.098 25.0 67.0 0.99680 3.20 0.68 9.8 5 1 2 7.8 0.760 0.04 2.3 0.092 15.0 54.0 0.99700 3.26 0.65 9.8 5 2 3 11.2 0.280 0.56 1.9 0.075 17.0 60.0 0.99800 3.16 0.58 9.8 6 3 4 7.4 0.700 0.00 1.9 0.076 11.0 34.0 0.99780 3.51 0.56 9.4 5 4 ... ... ... ... ... ... ... ... ... ... ... ... ... ... 1138 6.3 0.510 0.13 2.3 0.076 29.0 40.0 0.99574 3.42 0.75 11.0 6 1592 1139 6.8 0.620 0.08 1.9 0.068 28.0 38.0 0.99651 3.42 0.82 9.5 6 1593 1140 6.2 0.600 0.08 2.0 0.090 32.0 44.0 0.99490 3.45 0.58 10.5 5 1594 1141 5.9 0.550 0.10 2.2 0.062 39.0 51.0 0.99512 3.52 0.76 11.2 6 1595 1142 5.9 0.645 0.12 2.0 0.075 32.0 44.0 0.99547 3.57 0.71 10.2 5 1597 <p>1143 rows \u00d7 13 columns</p> In\u00a0[18]: Copied! <pre>df.describe()\n</pre> df.describe() Out[18]: fixed acidity volatile acidity citric acid residual sugar chlorides free sulfur dioxide total sulfur dioxide density pH sulphates alcohol quality Id count 1143.000000 1143.000000 1143.000000 1143.000000 1143.000000 1143.000000 1143.000000 1143.000000 1143.000000 1143.000000 1143.000000 1143.000000 1143.000000 mean 8.311111 0.531339 0.268364 2.532152 0.086933 15.615486 45.914698 0.996730 3.311015 0.657708 10.442111 5.657043 804.969379 std 1.747595 0.179633 0.196686 1.355917 0.047267 10.250486 32.782130 0.001925 0.156664 0.170399 1.082196 0.805824 463.997116 min 4.600000 0.120000 0.000000 0.900000 0.012000 1.000000 6.000000 0.990070 2.740000 0.330000 8.400000 3.000000 0.000000 25% 7.100000 0.392500 0.090000 1.900000 0.070000 7.000000 21.000000 0.995570 3.205000 0.550000 9.500000 5.000000 411.000000 50% 7.900000 0.520000 0.250000 2.200000 0.079000 13.000000 37.000000 0.996680 3.310000 0.620000 10.200000 6.000000 794.000000 75% 9.100000 0.640000 0.420000 2.600000 0.090000 21.000000 61.000000 0.997845 3.400000 0.730000 11.100000 6.000000 1209.500000 max 15.900000 1.580000 1.000000 15.500000 0.611000 68.000000 289.000000 1.003690 4.010000 2.000000 14.900000 8.000000 1597.000000 In\u00a0[25]: Copied! <pre># X / Y Split\nY = df['quality']\nX = df.drop(['quality','Id'], axis=1)\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.3)\n\n# \ubaa8\ub378 import\nfrom sklearn.tree import DecisionTreeClassifier\n\n# \ubaa8\ub378 \uc0dd\uc131\ndt = DecisionTreeClassifier(max_depth=4)\ndt.fit(X_train, Y_train)\n\ndt\n</pre> # X / Y Split Y = df['quality'] X = df.drop(['quality','Id'], axis=1)  from sklearn.model_selection import train_test_split  X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.3)  # \ubaa8\ub378 import from sklearn.tree import DecisionTreeClassifier  # \ubaa8\ub378 \uc0dd\uc131 dt = DecisionTreeClassifier(max_depth=4) dt.fit(X_train, Y_train)  dt  Out[25]: <pre>DecisionTreeClassifier(max_depth=4)</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.DecisionTreeClassifier?Documentation for DecisionTreeClassifieriFitted<pre>DecisionTreeClassifier(max_depth=4)</pre> In\u00a0[27]: Copied! <pre>import matplotlib.pyplot as plt\nfrom sklearn.tree import plot_tree\n\nplot_tree(dt, feature_names=X.columns, filled=True, fontsize=5)\nplt.title(\"Decision Tree\")\nplt.show()\n</pre> import matplotlib.pyplot as plt from sklearn.tree import plot_tree  plot_tree(dt, feature_names=X.columns, filled=True, fontsize=5) plt.title(\"Decision Tree\") plt.show() In\u00a0[31]: Copied! <pre>import matplotlib.pyplot as plt\n\nplt.figure(figsize=(18, 6)) \nplt.bar(X.columns, dt.feature_importances_)\n</pre> import matplotlib.pyplot as plt  plt.figure(figsize=(18, 6))  plt.bar(X.columns, dt.feature_importances_) Out[31]: <pre>&lt;BarContainer object of 11 artists&gt;</pre> In\u00a0[33]: Copied! <pre>Y_pred = dt.predict(X_test)\nY_pred\n</pre> Y_pred = dt.predict(X_test) Y_pred Out[33]: <pre>array([5, 6, 6, 5, 6, 5, 6, 7, 5, 5, 5, 6, 6, 6, 5, 6, 5, 5, 7, 5, 5, 7,\n       5, 6, 6, 6, 6, 6, 6, 6, 6, 5, 5, 6, 6, 5, 7, 6, 7, 5, 6, 6, 6, 5,\n       6, 6, 6, 5, 5, 6, 5, 6, 5, 7, 5, 7, 6, 6, 5, 6, 5, 5, 5, 5, 5, 5,\n       6, 5, 7, 5, 5, 5, 6, 5, 6, 5, 5, 6, 6, 5, 6, 6, 6, 5, 5, 6, 6, 6,\n       5, 6, 5, 6, 6, 5, 6, 7, 5, 6, 5, 5, 6, 5, 5, 5, 6, 6, 5, 6, 5, 5,\n       6, 7, 5, 6, 5, 6, 6, 5, 5, 6, 5, 6, 5, 5, 6, 5, 5, 6, 6, 6, 5, 7,\n       5, 6, 5, 6, 5, 5, 7, 5, 6, 5, 6, 5, 5, 5, 5, 6, 5, 6, 6, 6, 6, 5,\n       5, 5, 7, 6, 6, 7, 5, 5, 7, 6, 5, 5, 5, 7, 6, 6, 7, 7, 6, 5, 6, 6,\n       5, 5, 7, 6, 5, 5, 6, 7, 6, 5, 6, 5, 5, 5, 5, 7, 5, 5, 5, 5, 5, 7,\n       5, 6, 5, 5, 5, 6, 7, 5, 5, 6, 5, 6, 5, 5, 6, 5, 7, 5, 5, 5, 6, 5,\n       6, 5, 7, 6, 6, 5, 7, 5, 7, 5, 5, 5, 5, 7, 6, 6, 5, 5, 6, 6, 5, 5,\n       6, 6, 5, 5, 5, 7, 5, 7, 5, 6, 6, 5, 5, 7, 6, 5, 5, 5, 5, 6, 5, 5,\n       5, 5, 7, 5, 5, 7, 7, 5, 6, 5, 6, 6, 5, 5, 7, 5, 6, 5, 5, 5, 5, 5,\n       6, 5, 5, 5, 5, 7, 5, 5, 5, 5, 7, 6, 7, 7, 5, 5, 6, 5, 7, 5, 6, 5,\n       5, 5, 7, 5, 5, 5, 7, 5, 5, 7, 5, 5, 5, 6, 5, 6, 5, 7, 7, 6, 5, 6,\n       5, 7, 6, 6, 7, 6, 6, 5, 7, 5, 7, 6, 6])</pre> In\u00a0[28]: Copied! <pre># \ubaa8\ub378 \ud3c9\uac00\uc9c0\ud45c \ucd9c\ub825\nfrom sklearn.metrics import classification_report\n\nprint(classification_report(Y_test,Y_pred))\n</pre> # \ubaa8\ub378 \ud3c9\uac00\uc9c0\ud45c \ucd9c\ub825 from sklearn.metrics import classification_report  print(classification_report(Y_test,Y_pred)) <pre>              precision    recall  f1-score   support\n\n           3       0.00      0.00      0.00         1\n           4       0.00      0.00      0.00         9\n           5       0.62      0.80      0.70       138\n           6       0.59      0.48      0.53       146\n           7       0.39      0.41      0.40        46\n           8       0.00      0.00      0.00         3\n\n    accuracy                           0.58       343\n   macro avg       0.27      0.28      0.27       343\nweighted avg       0.56      0.58      0.56       343\n\n</pre> <pre>/Users/jonhpark/workspace/courses_archive/mkdocs_venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n/Users/jonhpark/workspace/courses_archive/mkdocs_venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n/Users/jonhpark/workspace/courses_archive/mkdocs_venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[38]: Copied! <pre># \ubaa8\ub378 import\nfrom sklearn.ensemble import RandomForestClassifier\n\n# \ubaa8\ub378 \uc0dd\uc131\nrfr = RandomForestClassifier( max_depth=10, n_estimators = 100)\n\n# \ubaa8\ub378 \ud559\uc2b5\nrfr.fit(X_train, Y_train)\n</pre> # \ubaa8\ub378 import from sklearn.ensemble import RandomForestClassifier  # \ubaa8\ub378 \uc0dd\uc131 rfr = RandomForestClassifier( max_depth=10, n_estimators = 100)  # \ubaa8\ub378 \ud559\uc2b5 rfr.fit(X_train, Y_train) Out[38]: <pre>RandomForestClassifier(max_depth=10)</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomForestClassifier?Documentation for RandomForestClassifieriFitted<pre>RandomForestClassifier(max_depth=10)</pre> In\u00a0[39]: Copied! <pre>Y_pred = rfr.predict(X_test)\nprint(classification_report(Y_test,Y_pred))\n</pre> Y_pred = rfr.predict(X_test) print(classification_report(Y_test,Y_pred)) <pre>              precision    recall  f1-score   support\n\n           3       0.00      0.00      0.00         1\n           4       0.00      0.00      0.00         9\n           5       0.69      0.79      0.74       138\n           6       0.63      0.68      0.66       146\n           7       0.62      0.39      0.48        46\n           8       0.00      0.00      0.00         3\n\n    accuracy                           0.66       343\n   macro avg       0.32      0.31      0.31       343\nweighted avg       0.63      0.66      0.64       343\n\n</pre> <pre>/Users/jonhpark/workspace/courses_archive/mkdocs_venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n/Users/jonhpark/workspace/courses_archive/mkdocs_venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n/Users/jonhpark/workspace/courses_archive/mkdocs_venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n</pre>"},{"location":"machine_learning_tutorial/02_classification/#machine-learning-classification","title":"Machine Learning - Classification\u00b6","text":""},{"location":"machine_learning_tutorial/02_classification/#1","title":"1. \ubd84\ub958 \ubb38\uc81c\ub97c \ud480\uc5b4\ubd05\uc2dc\ub2e4!\u00b6","text":""},{"location":"machine_learning_tutorial/02_classification/#y-fx","title":"$$y = f(x)$$\u00b6","text":"<p>y \uac00 continous \ud558\uc9c0 \uc54a\uace0, discrete \ud55c \uacbd\uc6b0\uc785\ub2c8\ub2e4. \uce74\ud14c\uace0\ub9ac \uc911 \ud558\ub098\ub97c \ub9de\ucd98\ub2e4 \ub77c\uace0 \ubcf4\uba74 \ub429\ub2c8\ub2e4.</p> <p>\ucc38\uace0\ub85c, GPT \uc640 \uac19\uc740 LLM (Large Language Model) \ub3c4 Classification \ubb38\uc81c\ub97c \ud480\uc740 \ubaa8\ub378 \uc785\ub2c8\ub2e4.</p>"},{"location":"machine_learning_tutorial/02_classification/#2-tree-based-model","title":"2. Tree Based Model\u00b6","text":"<p>\ubd84\ub958 \ubaa8\ub378\uc5d0 \ub9ce\uc774 \uc0ac\uc6a9\ub418\ub294 Tree \uad6c\uc870\ub97c \uae30\ubc18\uc73c\ub85c\ud55c \ubaa8\ub378\uc744 \uc0ac\uc6a9\ud574 \ubcf4\uaca0\uc2b5\ub2c8\ub2e4. \ucc38\uace0\ub85c, Tree Based Model\uc740 Regression \uc5d0\ub3c4 \uc0ac\uc6a9\uc774 \uac00\ub2a5\ud569\ub2c8\ub2e4.</p>"},{"location":"machine_learning_tutorial/02_classification/#21-decision-tree","title":"2.1. \uae30\ubcf8 \ubaa8\ub378 - Decision Tree\u00b6","text":"<p>\uc544\ub798\uc758 \uadf8\ub798\ud504\ub97c \ubcf4\uba74 \uc774\ud574\uac00 \uc27d\uc2b5\ub2c8\ub2e4.</p>"},{"location":"machine_learning_tutorial/02_classification/#3-machine-learning","title":"3. Machine Learning \uc608\uc2dc\u00b6","text":"<ul> <li>\uc5ec\ub7ec \uc815\ubcf4\ub97c \uc774\uc6a9\ud574\uc11c \uc640\uc778\uc758 \ub4f1\uae09\uc744 \ub9de\ucdb0\ubcf4\uc790!</li> <li>data \ud3f4\ub354\uc758 WineQT.csv \ub85c \uc608\uc2dc</li> </ul> <p>\ucc38\uc870 - \uce90\uae00\uc758 \uc640\uc778 \ud488\uc9c8 \ub370\uc774\ud130 \uc5f4\ub78c</p>"},{"location":"machine_learning_tutorial/02_classification/#4","title":"4. \ud3c9\uac00\ud558\uae30\u00b6","text":"<ul> <li>\uc5bc\ub9c8\ub098 \ub9de\ucdc4\ub294\uc9c0 \uc218\uce58\ud654\ud558\uae30</li> </ul>"},{"location":"machine_learning_tutorial/02_classification/","title":"\uacb0\uacfc \ud574\uc11d!\u00b6","text":"<ol> <li>precision (\uc815\ubc00\ub3c4): \u2022\t\ubaa8\ub378\uc774 \ud2b9\uc815 \ud074\ub798\uc2a4\ub85c \uc608\uce21\ud55c \uac83 \uc911 \uc2e4\uc81c\ub85c \ub9de\uc740 \ube44\uc728. \u2022\t\uc608\ub97c \ub4e4\uc5b4, \ud074\ub798\uc2a4 5\uc5d0\uc11c 0.62\ub294 \ubaa8\ub378\uc774 \ud074\ub798\uc2a4 5\ub85c \uc608\uce21\ud55c \uac83 \uc911 62%\uac00 \uc2e4\uc81c\ub85c \ud074\ub798\uc2a4 5\uc600\ub2e4\ub294 \ub73b.</li> <li>recall (\uc7ac\ud604\uc728): \u2022\t\uc2e4\uc81c\ub85c \ud2b9\uc815 \ud074\ub798\uc2a4\uc778 \uac83 \uc911\uc5d0\uc11c \ubaa8\ub378\uc774 \ub9de\uac8c \uc608\uce21\ud55c \ube44\uc728. \u2022\t\ud074\ub798\uc2a4 5\uc5d0\uc11c 0.80\uc740 \uc2e4\uc81c \ud074\ub798\uc2a4 5\uc778 \ub370\uc774\ud130 \uc911 80%\ub97c \ub9de\ucdc4\ub2e4\ub294 \uc758\ubbf8.</li> <li>f1-score: \u2022\t\uc815\ubc00\ub3c4\uc640 \uc7ac\ud604\uc728\uc758 \uc870\ud654 \ud3c9\uade0\uc73c\ub85c, \ubaa8\ub378\uc758 \uc804\uccb4\uc801\uc778 \uc131\ub2a5\uc744 \ub098\ud0c0\ub0c4. \u2022\t\ud074\ub798\uc2a4 5\uc5d0\uc11c 0.70\uc740 \ud574\ub2f9 \ud074\ub798\uc2a4\uc5d0\uc11c \uc815\ubc00\ub3c4\uc640 \uc7ac\ud604\uc728\uc758 \uade0\ud615\uc774 \uc801\uc808\ud558\ub2e4\ub294 \ub73b.</li> <li>support: \u2022\t\uac01 \ud074\ub798\uc2a4\uc5d0 \uc18d\ud558\ub294 \uc2e4\uc81c \ub370\uc774\ud130\uc758 \uac1c\uc218. \u2022\t\uc608\ub97c \ub4e4\uc5b4, \ud074\ub798\uc2a4 5\uc758 support \uac12 138\uc740 \uc2e4\uc81c\ub85c \ud074\ub798\uc2a4 5\uc778 \ub370\uc774\ud130\uac00 138\uac1c\ub77c\ub294 \uac83\uc744 \uc758\ubbf8.</li> </ol> <p>\uac01 \ud074\ub798\uc2a4\ubcc4 \uc131\ub2a5 \ud574\uc11d</p> <ol> <li>\ud074\ub798\uc2a4 3, 4, 8: \u2022\t\uc815\ubc00\ub3c4, \uc7ac\ud604\uc728, f1-score\uac00 \ubaa8\ub450 0. \uc774\ub294 \ubaa8\ub378\uc774 \ud574\ub2f9 \ud074\ub798\uc2a4\uc5d0 \ub300\ud574 \uc804\ud600 \uc62c\ubc14\ub974\uac8c \uc608\uce21\ud558\uc9c0 \ubabb\ud588\ub2e4\ub294 \ub73b. \u2022\tsupport\uac00 \ub0ae\uc544\uc11c \ub370\uc774\ud130\uac00 \ubd80\uc871\ud588\uc744 \uac00\ub2a5\uc131\uc774 \ub192\uc74c.</li> <li>\ud074\ub798\uc2a4 5: \u2022\t\uc815\ubc00\ub3c4(0.62), \uc7ac\ud604\uc728(0.80), f1-score(0.70) \ubaa8\ub450 \ub192\uc74c. \ubaa8\ub378\uc774 \uc774 \ud074\ub798\uc2a4\uc5d0\uc11c\ub294 \ube44\uad50\uc801 \uc798 \uc791\ub3d9\ud568.</li> <li>\ud074\ub798\uc2a4 6: \u2022\t\uc815\ubc00\ub3c4(0.59), \uc7ac\ud604\uc728(0.48), f1-score(0.53). \uc131\ub2a5\uc774 \uc911\uac04 \uc815\ub3c4.</li> <li>\ud074\ub798\uc2a4 7: \u2022\t\uc815\ubc00\ub3c4(0.39), \uc7ac\ud604\uc728(0.41), f1-score(0.40). \uc131\ub2a5\uc774 \ub0ae\uc74c.</li> </ol> <p>\uc804\uccb4 \uc131\ub2a5</p> <ol> <li>accuracy (\uc815\ud655\ub3c4): \u2022\t\uc804\uccb4 \ub370\uc774\ud130 \uc911 \ubaa8\ub378\uc774 \ub9de\ucd98 \ube44\uc728. 58%\ub85c, \ubaa8\ub378\uc758 \uc131\ub2a5\uc774 \ubcf4\ud1b5 \uc218\uc900\uc784.</li> <li>macro avg (\ub9e4\ud06c\ub85c \ud3c9\uade0): \u2022\t\ubaa8\ub4e0 \ud074\ub798\uc2a4\uc758 \uc815\ubc00\ub3c4, \uc7ac\ud604\uc728, f1-score\uc758 \ub2e8\uc21c \ud3c9\uade0. \u2022\t\ud074\ub798\uc2a4 \uac04 \ub370\uc774\ud130 \ubd88\uade0\ud615\uc774 \uc2ec\ud55c \uacbd\uc6b0 \ub0ae\uc744 \uc218 \uc788\uc74c. \uc5ec\uae30\uc11c\ub294 \uac01\uac01 \uc57d 0.27 \uc815\ub3c4\ub85c \ub0ae\uc74c.</li> <li>weighted avg (\uac00\uc911 \ud3c9\uade0): \u2022\t\uac01 \ud074\ub798\uc2a4\uc758 support(\ub370\uc774\ud130 \uac1c\uc218)\ub97c \ubc18\uc601\ud558\uc5ec \uacc4\uc0b0\ub41c \ud3c9\uade0. \u2022\t\uc5ec\uae30\uc11c\ub294 accuracy\uc640 \ube44\uc2b7\ud55c 0.56~0.58 \uc218\uc900\uc73c\ub85c \ub098\ud0c0\ub0a8.</li> </ol>"},{"location":"machine_learning_tutorial/02_classification/#5-random-forest","title":"5. \ub2e4\ub978 \ubaa8\ub378 \uc0ac\uc6a9\ud558\uae30 - Random Forest\u00b6","text":"<ul> <li>Decesion Tree \ub4e4\uc758 \uc559\uc0c1\ube14 (Ensemble)!</li> </ul> <p>\ub9c8\uce58 \uc9d1\ub2e8 \uc9c0\uc131\ucc98\ub7fc, \uc5ec\ub7ec\uac00\uc9c0\uc758 decistion tree \uac00 \uc120\ud0dd\ud55c \uacb0\uacfc\ub4e4\uc744 \ud1a0\ub300\ub85c \ucd5c\uc885 \ub300\ub2f5\uc744 \ub3c4\ucd9c\ud568</p> <p></p>"},{"location":"machine_learning_tutorial/03_hparam_cv/","title":"Machine Learning - \ud558\uc774\ud37c \ud30c\ub77c\ubbf8\ud130 \ud29c\ub2dd &amp; \uad50\ucc28 \uac80\uc99d","text":"In\u00a0[2]: Copied! <pre># Insuarance Data - https://github.com/stedy/Machine-Learning-with-R-datasets/blob/master/insurance.csv\n\nimport pandas as pd\n\ndf = pd.read_csv(\"../../data/insurance.csv\")\ndf\n</pre> # Insuarance Data - https://github.com/stedy/Machine-Learning-with-R-datasets/blob/master/insurance.csv  import pandas as pd  df = pd.read_csv(\"../../data/insurance.csv\") df Out[2]: age sex bmi children smoker region charges 0 19 female 27.900 0 yes southwest 16884.92400 1 18 male 33.770 1 no southeast 1725.55230 2 28 male 33.000 3 no southeast 4449.46200 3 33 male 22.705 0 no northwest 21984.47061 4 32 male 28.880 0 no northwest 3866.85520 ... ... ... ... ... ... ... ... 1333 50 male 30.970 3 no northwest 10600.54830 1334 18 female 31.920 0 no northeast 2205.98080 1335 18 female 36.850 0 no southeast 1629.83350 1336 21 female 25.800 0 no southwest 2007.94500 1337 61 female 29.070 0 yes northwest 29141.36030 <p>1338 rows \u00d7 7 columns</p> In\u00a0[3]: Copied! <pre># VISUALIZE!!\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# 1. Pairplot for numerical relationships, highlighting smoker status\nsns.set(style=\"whitegrid\")\nsns.pairplot(\n    df, \n    hue=\"smoker\", \n    vars=[\"age\", \"bmi\", \"children\", \"charges\"], \n    diag_kind=\"hist\",\n    palette=\"muted\"\n)\nplt.suptitle(\"Pairplot of Key Features (Colored by Smoker)\", y=1.02)\nplt.show()\n\n# 2. Barplot for charges by region and smoker status\nplt.figure(figsize=(10, 6))\nsns.barplot(\n    x=\"region\", \n    y=\"charges\", \n    hue=\"smoker\", \n    data=df, \n    ci=None, \n    palette=\"pastel\"\n)\nplt.title(\"Average Charges by Region and Smoking Status\")\nplt.show()\n\n# 3. Distribution of charges with smoker categorization\nplt.figure(figsize=(10, 6))\nsns.histplot(\n    df, \n    x=\"charges\", \n    hue=\"smoker\", \n    kde=True, \n    palette=\"coolwarm\", \n    bins=30\n)\nplt.title(\"Distribution of Charges (Smoker vs Non-Smoker)\")\nplt.show()\n\n# 4. Scatterplot: BMI vs. Charges with regression and smoker distinction\nsns.lmplot(\n    x=\"bmi\", \n    y=\"charges\", \n    hue=\"smoker\", \n    data=df, \n    height=6, \n    aspect=1.5, \n    scatter_kws={\"alpha\": 0.5}\n)\nplt.title(\"BMI vs. Charges (Colored by Smoker)\")\nplt.show()\n</pre> # VISUALIZE!! import seaborn as sns import matplotlib.pyplot as plt  # 1. Pairplot for numerical relationships, highlighting smoker status sns.set(style=\"whitegrid\") sns.pairplot(     df,      hue=\"smoker\",      vars=[\"age\", \"bmi\", \"children\", \"charges\"],      diag_kind=\"hist\",     palette=\"muted\" ) plt.suptitle(\"Pairplot of Key Features (Colored by Smoker)\", y=1.02) plt.show()  # 2. Barplot for charges by region and smoker status plt.figure(figsize=(10, 6)) sns.barplot(     x=\"region\",      y=\"charges\",      hue=\"smoker\",      data=df,      ci=None,      palette=\"pastel\" ) plt.title(\"Average Charges by Region and Smoking Status\") plt.show()  # 3. Distribution of charges with smoker categorization plt.figure(figsize=(10, 6)) sns.histplot(     df,      x=\"charges\",      hue=\"smoker\",      kde=True,      palette=\"coolwarm\",      bins=30 ) plt.title(\"Distribution of Charges (Smoker vs Non-Smoker)\") plt.show()  # 4. Scatterplot: BMI vs. Charges with regression and smoker distinction sns.lmplot(     x=\"bmi\",      y=\"charges\",      hue=\"smoker\",      data=df,      height=6,      aspect=1.5,      scatter_kws={\"alpha\": 0.5} ) plt.title(\"BMI vs. Charges (Colored by Smoker)\") plt.show() <pre>/var/folders/kx/c6xk17ln6blbs7f3nf_p1str0000gn/T/ipykernel_33742/3579438954.py:19: FutureWarning: \n\nThe `ci` parameter is deprecated. Use `errorbar=None` for the same effect.\n\n  sns.barplot(\n</pre> In\u00a0[4]: Copied! <pre>from sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.metrics import mean_squared_error, r2_score\n\n# Prepare the data for the Decision Tree Regressor\n# Convert categorical variables into dummy variables\ninsurance_data_encoded = pd.get_dummies(df, drop_first=True)\n\n# Define features (X) and target (y)\nX = insurance_data_encoded.drop(\"charges\", axis=1)\ny = insurance_data_encoded[\"charges\"]\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n\n# Create and train the Decision Tree Regressor\ndecision_tree_model = DecisionTreeRegressor(random_state=42)\ndecision_tree_model.fit(X_train, y_train)\n\n# Make predictions on the test set\ny_pred = decision_tree_model.predict(X_test)\n\n# Evaluate the model\nmse = mean_squared_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\n\nmse, r2\n</pre> from sklearn.model_selection import train_test_split from sklearn.tree import DecisionTreeRegressor from sklearn.metrics import mean_squared_error, r2_score  # Prepare the data for the Decision Tree Regressor # Convert categorical variables into dummy variables insurance_data_encoded = pd.get_dummies(df, drop_first=True)  # Define features (X) and target (y) X = insurance_data_encoded.drop(\"charges\", axis=1) y = insurance_data_encoded[\"charges\"]  # Split the data into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)  # Create and train the Decision Tree Regressor decision_tree_model = DecisionTreeRegressor(random_state=42) decision_tree_model.fit(X_train, y_train)  # Make predictions on the test set y_pred = decision_tree_model.predict(X_test)  # Evaluate the model mse = mean_squared_error(y_test, y_pred) r2 = r2_score(y_test, y_pred)  mse, r2 Out[4]: <pre>(44727632.28565265, 0.672863521755465)</pre> In\u00a0[5]: Copied! <pre>from sklearn.model_selection import GridSearchCV\n\n# Define a reduced parameter grid for efficient tuning\nparam_grid = {\n    'max_depth': [None, 10, 20, 30],\n    'min_samples_split': [2, 10, 20],\n    'min_samples_leaf': [1, 5, 10]\n}\n\n# Perform hyperparameter tuning using GridSearchCV\ngrid_search = GridSearchCV(\n    DecisionTreeRegressor(),\n    param_grid,\n    cv=3,\n    verbose=1,\n    n_jobs=-1\n)\n\ngrid_search.fit(X_train, y_train)\n\n# Best parameters and results\nbest_params = grid_search.best_params_\nbest_model = grid_search.best_estimator_\n\n# Evaluate the tuned model on the test set\ny_pred = best_model.predict(X_test)\nmse = mean_squared_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\n\nbest_params, mse, r2\n</pre> from sklearn.model_selection import GridSearchCV  # Define a reduced parameter grid for efficient tuning param_grid = {     'max_depth': [None, 10, 20, 30],     'min_samples_split': [2, 10, 20],     'min_samples_leaf': [1, 5, 10] }  # Perform hyperparameter tuning using GridSearchCV grid_search = GridSearchCV(     DecisionTreeRegressor(),     param_grid,     cv=3,     verbose=1,     n_jobs=-1 )  grid_search.fit(X_train, y_train)  # Best parameters and results best_params = grid_search.best_params_ best_model = grid_search.best_estimator_  # Evaluate the tuned model on the test set y_pred = best_model.predict(X_test) mse = mean_squared_error(y_test, y_pred) r2 = r2_score(y_test, y_pred)  best_params, mse, r2 <pre>Fitting 3 folds for each of 36 candidates, totalling 108 fits\n</pre> Out[5]: <pre>({'max_depth': None, 'min_samples_leaf': 10, 'min_samples_split': 2},\n 24087554.05824817,\n 0.823824396654508)</pre> In\u00a0[7]: Copied! <pre>import matplotlib.pyplot as plt\nfrom sklearn.tree import plot_tree\n\nplot_tree(decision_tree_model, feature_names=X.columns, filled=True, fontsize=5)\nplt.title(\"Default Decision Tree\")\nplt.show()\n\n\nplot_tree(best_model, feature_names=X.columns, filled=True, fontsize=5)\nplt.title(\"Best Decision Tree\")\nplt.show()\n</pre> import matplotlib.pyplot as plt from sklearn.tree import plot_tree  plot_tree(decision_tree_model, feature_names=X.columns, filled=True, fontsize=5) plt.title(\"Default Decision Tree\") plt.show()   plot_tree(best_model, feature_names=X.columns, filled=True, fontsize=5) plt.title(\"Best Decision Tree\") plt.show() In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"machine_learning_tutorial/03_hparam_cv/#machine-learning-","title":"Machine Learning - \ud558\uc774\ud37c \ud30c\ub77c\ubbf8\ud130 \ud29c\ub2dd &amp; \uad50\ucc28 \uac80\uc99d\u00b6","text":"<p>\uad81\uadf9\uc801\uc778 \ubaa9\ud45c: \uc88b\uc740 \uba38\uc2e0\ub7ec\ub2dd \ubaa8\ub378\uc744 \ub9cc\ub4e4\uc790!</p> <ul> <li>\ubaa8\ub378\uc744 \uc120\ud0dd\ud588\uc73c\uba74, \ud558\uc774\ud37c \ud30c\ub77c\ubbf8\ud130\ub97c \ud29c\ub2dd\ud574\uc11c \ub354 \uc88b\uc740 \ubaa8\ub378\uc744 \ub9cc\ub4e4\uc5b4\ubd05\uc2dc\ub2e4.</li> <li>\ub354 \uc88b\uc740 \ubaa8\ub378\uc740 \uc5b4\ub5bb\uac8c \ud3c9\uac00\ud574\uc57c \uc81c\uc77c \uc88b\uc744\uae4c\uc694?</li> </ul>"},{"location":"machine_learning_tutorial/03_hparam_cv/#1-hyper-parameter","title":"1. Hyper Parameter\u00b6","text":"<ul> <li>\ud558\uc774\ud37c \ud30c\ub77c\ubbf8\ud130 = \ubaa8\ub378\uc774 \ub9cc\ub4e4\uc5b4\uc9c0\ub294 \ub370\uc5d0 \uc5f0\uad6c\uc790\uac00 \ubcc0\uacbd\ud560 \uc218 \uc788\ub294 \uc694\uc18c\ub4e4 \ubaa8\ub450 \ub2e4.</li> <li>Decision Tree \uc758 \uc608\uc2dc\uc5d0\uc11c\ub294 \uae4a\uc774. \ucd5c\ub300 \ub178\ub4dc\uc758 \uac2f\uc218, \uc0ac\uc6a9\ud560 feature\uc758 \uac2f\uc218, ...</li> </ul> <p>\ud558\uc774\ud37c \ud30c\ub77c\ubbf8\ud130\ub97c \ubcc0\uacbd\ud558\uba74\uc11c \uc9c1\uc811 \ubaa8\ub378\uc744 \ud29c\ub2dd\ud574 \ubd05\uc2dc\ub2e4!</p>"},{"location":"machine_learning_tutorial/03_hparam_cv/","title":"\ud558\uc774\ud37c \ud30c\ub77c\ubbf8\ud130 \ud29c\ub2dd \ud558\uae30\u00b6","text":"<p>\ud2b9\ud788 Tree Based model\uc740 \uac00\ub2a5\ud55c \ud30c\ub77c\ubbf8\ud130 \uc870\ud569\ub530\ub77c\uc11c \ub9ce\uc740 \uc131\ub2a5\ucc28\uc774\ub97c \ubcf4\uc774\uace4 \ud569\ub2c8\ub2e4. \ub2e4\uc591\ud55c \ud30c\ub77c\ubbf8\ud130\ub4e4\uc744 \uc2e4\ud5d8\ud574 \ubcf4\uba74\uc11c \uc81c\uc77c \uc88b\uc740 \ubaa8\ub378\uc744 \ucc3e\uc544\ubd05\uc2dc\ub2e4.</p>"},{"location":"machine_learning_tutorial/03_hparam_cv/#cross-validation","title":"\uad50\ucc28 \uac80\uc99d (Cross Validation)\u00b6","text":"<p>\ud559\uc2b5\uc2dc\uc5d0\ub294 k-fold, N \ub354\ubbf8\ub85c \ub370\uc774\ud130\ub97c \ub098\ub220\uc11c \ud559\uc2b5/\uc2e4\ud5d8\uc744 \ubc18\ubcf5\ud569\ub2c8\ub2e4. \uc65c\ub0d0\ud558\uba74, \ub79c\ub364\ud558\uac8c \ub098\ub220\uc9c4 \ub370\uc774\ud130\uc5d0\uc11c \uc6b0\uc5f0\ud788 \ubaa8\ub378\uc774 \uc88b\uc744 \uc218\ub3c4 \uc788\uae30 \ub54c\ubb38\uc774\uc8e0. \uc5c4\uccad\ub098\uac8c \ub9ce\uc740 \ud30c\ub77c\ubbf8\ud130\ub97c \uc2dc\ud5d8\ud558\uac8c \ub418\uba74, overfit \uac00\ub2a5\uc131\uc774 \uc788\uae30 \ub54c\ubb38\uc5d0 \uc774\ub7f0 \uad50\ucc28 \uac80\uc99d \uacfc\uc815\uc744 \uac70\uce69\ub2c8\ub2e4.</p> <p>\ub525\ub7ec\ub2dd \uc138\uc0c1\uc73c\ub85c \uac00\uac8c \ub418\uba74, \ubcf4\ud1b5 CV \ub294 \ud558\uc9c0 \uc54a\uace0, train / valid / test \ub85c \ub098\ub220\uc11c \ud55c\ubc88\ub9cc \ud569\ub2c8\ub2e4. \uc65c\ub0d0\ud558\uba74 \ud559\uc2b5\uc774 \ub108\ubb34 \uc624\ub798\uac78\ub9ac\uace0 \ube44\uc2f8\ub2c8\uae4c\uc694...</p>"},{"location":"python_tutorial/01_Python_Basic/","title":"Python \uae30\ucd08","text":"In\u00a0[3]: Copied! <pre># COMMENT  : \uc8fc\uc11d \uc785\ub2c8\ub2e4.\n\n# pandas \ub97c \uc124\uce58\ud569\ub2c8\ub2e4.\n%pip install -U pandas\n\n# Python \ucf54\ub4dc \uc785\ub2c8\ub2e4.\nprint(\"HELLO WORLD!\")\n</pre> # COMMENT  : \uc8fc\uc11d \uc785\ub2c8\ub2e4.  # pandas \ub97c \uc124\uce58\ud569\ub2c8\ub2e4. %pip install -U pandas  # Python \ucf54\ub4dc \uc785\ub2c8\ub2e4. print(\"HELLO WORLD!\") <pre>Requirement already satisfied: pandas in /Users/jonhpark/workspace/courses_archive/mkdocs_venv/lib/python3.12/site-packages (2.2.2)\nCollecting pandas\n  Using cached pandas-2.2.3-cp312-cp312-macosx_11_0_arm64.whl.metadata (89 kB)\nRequirement already satisfied: numpy&gt;=1.26.0 in /Users/jonhpark/workspace/courses_archive/mkdocs_venv/lib/python3.12/site-packages (from pandas) (2.0.1)\nRequirement already satisfied: python-dateutil&gt;=2.8.2 in /Users/jonhpark/workspace/courses_archive/mkdocs_venv/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\nRequirement already satisfied: pytz&gt;=2020.1 in /Users/jonhpark/workspace/courses_archive/mkdocs_venv/lib/python3.12/site-packages (from pandas) (2024.1)\nRequirement already satisfied: tzdata&gt;=2022.7 in /Users/jonhpark/workspace/courses_archive/mkdocs_venv/lib/python3.12/site-packages (from pandas) (2024.1)\nRequirement already satisfied: six&gt;=1.5 in /Users/jonhpark/workspace/courses_archive/mkdocs_venv/lib/python3.12/site-packages (from python-dateutil&gt;=2.8.2-&gt;pandas) (1.16.0)\nUsing cached pandas-2.2.3-cp312-cp312-macosx_11_0_arm64.whl (11.4 MB)\nInstalling collected packages: pandas\n  Attempting uninstall: pandas\n    Found existing installation: pandas 2.2.2\n    Uninstalling pandas-2.2.2:\n      Successfully uninstalled pandas-2.2.2\nSuccessfully installed pandas-2.2.3\n\n[notice] A new release of pip is available: 24.0 -&gt; 24.3.1\n[notice] To update, run: pip install --upgrade pip\nNote: you may need to restart the kernel to use updated packages.\nHELLO WORLD!\n</pre> In\u00a0[4]: Copied! <pre>12 * 25\n</pre> 12 * 25 Out[4]: <pre>300</pre> In\u00a0[5]: Copied! <pre>str1 = 'happy'\nstr2 = 'new year!'\nstr1 + str2\n</pre> str1 = 'happy' str2 = 'new year!' str1 + str2 Out[5]: <pre>'happynew year!'</pre> In\u00a0[6]: Copied! <pre>int1 = 10\nint1 += 100\nint1\n</pre> int1 = 10 int1 += 100 int1 Out[6]: <pre>110</pre> In\u00a0[7]: Copied! <pre>x = 200\ny = \"2\ubc31\"\n\nprint(f'x\ub294 {x}\uc774\uace0, y\ub294 {y}\uc774\ub2e4')\n</pre> x = 200 y = \"2\ubc31\"  print(f'x\ub294 {x}\uc774\uace0, y\ub294 {y}\uc774\ub2e4') <pre>x\ub294 200\uc774\uace0, y\ub294 2\ubc31\uc774\ub2e4\n</pre> In\u00a0[8]: Copied! <pre>type(x)\n</pre> type(x) Out[8]: <pre>int</pre> In\u00a0[9]: Copied! <pre>type(y)\n</pre> type(y) Out[9]: <pre>str</pre> In\u00a0[10]: Copied! <pre>x == y\n</pre> x == y Out[10]: <pre>False</pre> In\u00a0[11]: Copied! <pre>type(x!=y)\n</pre> type(x!=y) Out[11]: <pre>bool</pre> In\u00a0[12]: Copied! <pre>from datetime import datetime\nwith open('log.txt', 'w') as f:\n    time1 = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n    print(f'{time1} - comments', file=f)\n</pre> from datetime import datetime with open('log.txt', 'w') as f:     time1 = datetime.now().strftime('%Y-%m-%d %H:%M:%S')     print(f'{time1} - comments', file=f) In\u00a0[13]: Copied! <pre>stocks = ['Apple', 'NVIDIA', 'Microsoft', 'Google', 'Amazon']\nstocks\n</pre> stocks = ['Apple', 'NVIDIA', 'Microsoft', 'Google', 'Amazon'] stocks Out[13]: <pre>['Apple', 'NVIDIA', 'Microsoft', 'Google', 'Amazon']</pre> In\u00a0[14]: Copied! <pre>prices = [258.2, 140.22, 439.33, \"197.57\", \"\uc774\ubc31\uc774\uc2ed\uad6c\"]\n</pre> prices = [258.2, 140.22, 439.33, \"197.57\", \"\uc774\ubc31\uc774\uc2ed\uad6c\"] In\u00a0[15]: Copied! <pre>type(prices)\n</pre> type(prices) Out[15]: <pre>list</pre> In\u00a0[16]: Copied! <pre>stocks_with_prices = {'Apple':258.2,\n                       'NVIDIA':140.22,\n                       'Microsoft':439.33,\n                       'Google':\"197.57\",\n                       \"Amazon\":\"\uc774\ubc31\uc774\uc2ed\uad6c\"}\nstocks_with_prices\n</pre> stocks_with_prices = {'Apple':258.2,                        'NVIDIA':140.22,                        'Microsoft':439.33,                        'Google':\"197.57\",                        \"Amazon\":\"\uc774\ubc31\uc774\uc2ed\uad6c\"} stocks_with_prices Out[16]: <pre>{'Apple': 258.2,\n 'NVIDIA': 140.22,\n 'Microsoft': 439.33,\n 'Google': '197.57',\n 'Amazon': '\uc774\ubc31\uc774\uc2ed\uad6c'}</pre> In\u00a0[17]: Copied! <pre>type(stocks_with_prices)\n</pre> type(stocks_with_prices) Out[17]: <pre>dict</pre> In\u00a0[\u00a0]: Copied! <pre>stocks[2]\n</pre> stocks[2] In\u00a0[\u00a0]: Copied! <pre>stocks[1:3]\n</pre> stocks[1:3] In\u00a0[\u00a0]: Copied! <pre>stocks[2] = 'Alphabet'\n</pre> stocks[2] = 'Alphabet' In\u00a0[\u00a0]: Copied! <pre>stocks.append('Tesla')\n</pre> stocks.append('Tesla') In\u00a0[\u00a0]: Copied! <pre>stocks_with_prices['Google']\n</pre> stocks_with_prices['Google'] In\u00a0[\u00a0]: Copied! <pre>stocks_with_prices.keys()\n</pre> stocks_with_prices.keys() In\u00a0[\u00a0]: Copied! <pre>stocks_with_prices.values()\n</pre> stocks_with_prices.values() In\u00a0[\u00a0]: Copied! <pre>if prices[0] &gt; 100:\n    print(\"\ube44\uc2fc\ub370?\")\n</pre> if prices[0] &gt; 100:     print(\"\ube44\uc2fc\ub370?\") In\u00a0[\u00a0]: Copied! <pre>if prices[0] &gt; 100:\n    print(\"\ube44\uc2fc\ub370?\")\nelif prices[0] &gt; 50:\n    print(\"\uc0b4\uae4c?\")\nelse:\n    print(\"\uc774\uac74 \uc0ac\uc57c\ud574!\")\n</pre> if prices[0] &gt; 100:     print(\"\ube44\uc2fc\ub370?\") elif prices[0] &gt; 50:     print(\"\uc0b4\uae4c?\") else:     print(\"\uc774\uac74 \uc0ac\uc57c\ud574!\") In\u00a0[\u00a0]: Copied! <pre>for i in range(5):\n    print(stocks[i])\n</pre> for i in range(5):     print(stocks[i]) In\u00a0[\u00a0]: Copied! <pre>for stock in stocks:\n    print(stock)\n</pre> for stock in stocks:     print(stock) <p>[\uc2e4\uc2b5] \uc885\ubaa9 \uc774\ub984\uacfc \uac00\uaca9\uc744 print \ud558\uace0, 100\ub2ec\ub7ec\uac00 \ub118\uc73c\uba74 \ube44\uc2f8\ub2e4\uace0, \uc801\uc73c\uba74 \uc2f8\ub2e4\uace0 print \ud574\ubcf4\uc138\uc694.</p> <p>type \uad00\ub828 \uc5d0\ub7ec\uac00 \ubc1c\uc0dd\ud560 \ud14c\ub2c8, \uc790\uc720\ub86d\uac8c \ud574\uacb0\ud574 \ubcf4\uc138\uc694.</p> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"python_tutorial/01_Python_Basic/#python","title":"Python \uae30\ucd08\u00b6","text":"<p>\uc815\ub9d0 \uc815\ub9d0 \uac04\ub2e8\ud55c \ucd5c\uc18c\ud55c\uc758 \ubb38\ubc95\ub9cc \ub2e4\ub8f9\ub2c8\ub2e4.</p> <p>Python \uc744 \uc774\ubbf8 \uc54c\uace0 \uacc4\uc2e0 \ubd84\ub4e4\uc740 \ub118\uc5b4\uac00\uc154\ub3c4 \uc88b\uc2b5\ub2c8\ub2e4.</p> <p>\ucc98\uc74c \ubc30\uc6b0\uc2dc\ub294 \ubd84\ub4e4\uc744 \uc704\ud574, \uc2e4\uc2b5\uc758 \uc2dc\uc791\uc810\uc774 \ub418\uae30 \uc704\ud558\uc5ec \ud544\uc218\uc801\uc778 \ubd80\ubd84\ub9cc \uac00\ubccd\uac8c \ub2e4\ub904\ubcf4\uaca0\uc2b5\ub2c8\ub2e4.</p>"},{"location":"python_tutorial/01_Python_Basic/#1-code-comment","title":"1. Code &amp; Comment\u00b6","text":"<ul> <li>\" # \" \uc740 \uc8fc\uc11d (Comment) \uc785\ub2c8\ub2e4. \ucf54\ub4dc\ub85c\uc11c\uc758 \ub3d9\uc791\uc740 \ubb34\uc2dc\ub429\ub2c8\ub2e4.</li> <li>Jupyter \uc640 \uac19\uc740 ipynb \uc758 \uacbd\uc6b0, ! \ub610\ub294 % \ub294 Python \ucf54\ub4dc\uac00 \uc544\ub2c8\ub77c terminal \uc5d0 \uc785\ub825 \ud558\ub294 \ub4f1\uc758 \ucef4\ud4e8\ud130\ub97c \uc81c\uc5b4\ud558\ub3c4\ub85d \uc804\ub2ec\ud569\ub2c8\ub2e4.</li> </ul>"},{"location":"python_tutorial/01_Python_Basic/#2-python","title":"2. Python \ub370\uc774\ud130 \ud0c0\uc785\u00b6","text":"<p>\uc22b\uc790, \ubb38\uc790, \ub9ac\uc2a4\ud2b8, \ub515\uc154\ub108\ub9ac \uc640 \uac19\uc740 \ub370\uc774\ud130 \ud0c0\uc785\uc5d0 \ub300\ud574\uc11c \uc54c\uc544\ubd05\ub2c8\ub2e4.</p>"},{"location":"python_tutorial/01_Python_Basic/#21-int-float-string-bool","title":"2.1. Int, Float, String, Bool\u00b6","text":"<p>\uc815\uc218, \uc18c\uc218, \ubb38\uc790\uc5f4 \ubaa8\ub450 \ub370\uc774\ud130 \ud0c0\uc785\uc785\ub2c8\ub2e4.</p>"},{"location":"python_tutorial/01_Python_Basic/#22-list-dictionary","title":"2.2. List &amp; Dictionary\u00b6","text":"<p>\uc5ec\ub7ec \uac1c\uc758 data\ub97c \uc800\uc7a5\ud558\ub294 List \uc640 dictionary \ub97c \uc54c\uc544\ubd05\uc2dc\ub2e4.</p>"},{"location":"python_tutorial/01_Python_Basic/#23-list-dictionary","title":"2.3. List &amp; Dictionary \ud65c\uc6a9\ud558\uae30\u00b6","text":"<ul> <li>\uc778\ub371\uc2f1</li> <li>\uc8fc\uc694 \uba54\uc18c\ub4dc \ud65c\uc6a9\ud558\uae30</li> </ul>"},{"location":"python_tutorial/01_Python_Basic/#3-ifelse-for","title":"3. IF/ELSE , FOR\u00b6","text":"<ul> <li>\uc870\uac74\ubb38, if/else \ub97c \uc0ac\uc6a9\ud574\ubd05\uc2dc\ub2e4.</li> <li>\ubc18\ubcf5\ubb38, for loop\uc744 \uc0ac\uc6a9\ud574\ubd05\uc2dc\ub2e4.</li> </ul>"},{"location":"python_tutorial/02_Data_Gathering/","title":"Data \ubc1b\uae30","text":"In\u00a0[\u00a0]: Copied! <pre># install yfinance library\n\n%pip install -U yfinance\n</pre> # install yfinance library  %pip install -U yfinance In\u00a0[2]: Copied! <pre>import yfinance as yf\n\n# Fetch historical data for a specific stock\ndata = yf.download(\"AAPL\", start=\"2023-01-01\", end=\"2023-12-31\")\n\ndata.head()  # View the first few rows\n</pre> import yfinance as yf  # Fetch historical data for a specific stock data = yf.download(\"AAPL\", start=\"2023-01-01\", end=\"2023-12-31\")  data.head()  # View the first few rows <pre>[*********************100%***********************]  1 of 1 completed</pre> <pre>Price            Close        High         Low        Open     Volume\nTicker            AAPL        AAPL        AAPL        AAPL       AAPL\nDate                                                                 \n2023-01-03  123.768456  129.537780  122.877820  128.924237  112117500\n2023-01-04  125.045036  127.321104  123.778358  125.569520   89113600\n2023-01-05  123.718979  126.440361  123.461690  125.807022   80962700\n2023-01-06  128.271103  128.934129  123.590330  124.698677   87754700\n2023-01-09  128.795578  132.021662  128.538289  129.112255   70790800\n</pre> <pre>\n</pre> In\u00a0[6]: Copied! <pre># Create a Ticker object\nticker = yf.Ticker(\"AAPL\")\n\n# Get stock info\ninfo = ticker.info\nprint(info)\n\n# Historical market data\nhist = ticker.history(period=\"1mo\")  # Data for the last 1 month\nhist\n</pre> # Create a Ticker object ticker = yf.Ticker(\"AAPL\")  # Get stock info info = ticker.info print(info)  # Historical market data hist = ticker.history(period=\"1mo\")  # Data for the last 1 month hist <pre>{'address1': 'One Apple Park Way', 'city': 'Cupertino', 'state': 'CA', 'zip': '95014', 'country': 'United States', 'phone': '(408) 996-1010', 'website': 'https://www.apple.com', 'industry': 'Consumer Electronics', 'industryKey': 'consumer-electronics', 'industryDisp': 'Consumer Electronics', 'sector': 'Technology', 'sectorKey': 'technology', 'sectorDisp': 'Technology', 'longBusinessSummary': 'Apple Inc. designs, manufactures, and markets smartphones, personal computers, tablets, wearables, and accessories worldwide. The company offers iPhone, a line of smartphones; Mac, a line of personal computers; iPad, a line of multi-purpose tablets; and wearables, home, and accessories comprising AirPods, Apple TV, Apple Watch, Beats products, and HomePod. It also provides AppleCare support and cloud services; and operates various platforms, including the App Store that allow customers to discover and download applications and digital content, such as books, music, video, games, and podcasts, as well as advertising services include third-party licensing arrangements and its own advertising platforms. In addition, the company offers various subscription-based services, such as Apple Arcade, a game subscription service; Apple Fitness+, a personalized fitness service; Apple Music, which offers users a curated listening experience with on-demand radio stations; Apple News+, a subscription news and magazine service; Apple TV+, which offers exclusive original content; Apple Card, a co-branded credit card; and Apple Pay, a cashless payment service, as well as licenses its intellectual property. The company serves consumers, and small and mid-sized businesses; and the education, enterprise, and government markets. It distributes third-party applications for its products through the App Store. The company also sells its products through its retail and online stores, and direct sales force; and third-party cellular network carriers, wholesalers, retailers, and resellers. Apple Inc. was founded in 1976 and is headquartered in Cupertino, California.', 'fullTimeEmployees': 164000, 'companyOfficers': [{'maxAge': 1, 'name': 'Mr. Timothy D. Cook', 'age': 62, 'title': 'CEO &amp; Director', 'yearBorn': 1961, 'fiscalYear': 2023, 'totalPay': 16239562, 'exercisedValue': 0, 'unexercisedValue': 0}, {'maxAge': 1, 'name': 'Mr. Luca  Maestri', 'age': 60, 'title': 'CFO &amp; Senior VP', 'yearBorn': 1963, 'fiscalYear': 2023, 'totalPay': 4612242, 'exercisedValue': 0, 'unexercisedValue': 0}, {'maxAge': 1, 'name': 'Mr. Jeffrey E. Williams', 'age': 59, 'title': 'Chief Operating Officer', 'yearBorn': 1964, 'fiscalYear': 2023, 'totalPay': 4637585, 'exercisedValue': 0, 'unexercisedValue': 0}, {'maxAge': 1, 'name': 'Ms. Katherine L. Adams', 'age': 59, 'title': 'Senior VP, General Counsel &amp; Secretary', 'yearBorn': 1964, 'fiscalYear': 2023, 'totalPay': 4618064, 'exercisedValue': 0, 'unexercisedValue': 0}, {'maxAge': 1, 'name': \"Ms. Deirdre  O'Brien\", 'age': 56, 'title': 'Chief People Officer &amp; Senior VP of Retail', 'yearBorn': 1967, 'fiscalYear': 2023, 'totalPay': 4613369, 'exercisedValue': 0, 'unexercisedValue': 0}, {'maxAge': 1, 'name': 'Mr. Chris  Kondo', 'title': 'Senior Director of Corporate Accounting', 'fiscalYear': 2023, 'exercisedValue': 0, 'unexercisedValue': 0}, {'maxAge': 1, 'name': 'Suhasini  Chandramouli', 'title': 'Director of Investor Relations', 'fiscalYear': 2023, 'exercisedValue': 0, 'unexercisedValue': 0}, {'maxAge': 1, 'name': 'Mr. Greg  Joswiak', 'title': 'Senior Vice President of Worldwide Marketing', 'fiscalYear': 2023, 'exercisedValue': 0, 'unexercisedValue': 0}, {'maxAge': 1, 'name': 'Mr. Adrian  Perica', 'age': 49, 'title': 'Head of Corporate Development', 'yearBorn': 1974, 'fiscalYear': 2023, 'exercisedValue': 0, 'unexercisedValue': 0}, {'maxAge': 1, 'name': 'Mr. Michael  Fenger', 'title': 'VP of Worldwide Sales', 'fiscalYear': 2023, 'exercisedValue': 0, 'unexercisedValue': 0}], 'auditRisk': 6, 'boardRisk': 1, 'compensationRisk': 2, 'shareHolderRightsRisk': 1, 'overallRisk': 1, 'governanceEpochDate': 1733011200, 'compensationAsOfEpochDate': 1703980800, 'irWebsite': 'http://investor.apple.com/', 'maxAge': 86400, 'priceHint': 2, 'previousClose': 255.27, 'open': 255.37, 'dayLow': 255.31, 'dayHigh': 258.21, 'regularMarketPreviousClose': 255.27, 'regularMarketOpen': 255.37, 'regularMarketDayLow': 255.31, 'regularMarketDayHigh': 258.21, 'dividendRate': 1.0, 'dividendYield': 0.0039, 'exDividendDate': 1731024000, 'payoutRatio': 0.1612, 'fiveYearAvgDividendYield': 0.62, 'beta': 1.24, 'trailingPE': 42.537067, 'forwardPE': 31.076347, 'volume': 20965006, 'regularMarketVolume': 20965006, 'averageVolume': 44261774, 'averageVolume10days': 54343500, 'averageDailyVolume10Day': 54343500, 'bid': 258.01, 'ask': 268.27, 'bidSize': 200, 'askSize': 100, 'marketCap': 3902899748864, 'fiftyTwoWeekLow': 164.08, 'fiftyTwoWeekHigh': 258.21, 'priceToSalesTrailing12Months': 9.9809475, 'fiftyDayAverage': 235.914, 'twoHundredDayAverage': 211.7723, 'trailingAnnualDividendRate': 0.98, 'trailingAnnualDividendYield': 0.0038390723, 'currency': 'USD', 'enterpriseValue': 3900713730048, 'profitMargins': 0.23971, 'floatShares': 15091184209, 'sharesOutstanding': 15115799552, 'sharesShort': 154104746, 'sharesShortPriorMonth': 133040194, 'sharesShortPreviousMonthDate': 1730332800, 'dateShortInterest': 1732838400, 'sharesPercentSharesOut': 0.010199999, 'heldPercentInsiders': 0.02056, 'heldPercentInstitutions': 0.61926, 'shortRatio': 3.41, 'shortPercentOfFloat': 0.010199999, 'impliedSharesOutstanding': 15332100096, 'bookValue': 3.767, 'priceToBook': 68.54261, 'lastFiscalYearEnd': 1727481600, 'nextFiscalYearEnd': 1759017600, 'mostRecentQuarter': 1727481600, 'earningsQuarterlyGrowth': -0.358, 'netIncomeToCommon': 93736001536, 'trailingEps': 6.07, 'forwardEps': 8.31, 'lastSplitFactor': '4:1', 'lastSplitDate': 1598832000, 'enterpriseToRevenue': 9.975, 'enterpriseToEbitda': 28.967, '52WeekChange': 0.33381546, 'SandP52WeekChange': 0.26272178, 'lastDividendValue': 0.25, 'lastDividendDate': 1731024000, 'exchange': 'NMS', 'quoteType': 'EQUITY', 'symbol': 'AAPL', 'underlyingSymbol': 'AAPL', 'shortName': 'Apple Inc.', 'longName': 'Apple Inc.', 'firstTradeDateEpochUtc': 345479400, 'timeZoneFullName': 'America/New_York', 'timeZoneShortName': 'EST', 'uuid': '8b10e4ae-9eeb-3684-921a-9ab27e4d87aa', 'messageBoardId': 'finmb_24937', 'gmtOffSetMilliseconds': -18000000, 'currentPrice': 258.2, 'targetHighPrice': 300.0, 'targetLowPrice': 184.0, 'targetMeanPrice': 246.25833, 'targetMedianPrice': 250.0, 'recommendationMean': 1.89362, 'recommendationKey': 'buy', 'numberOfAnalystOpinions': 42, 'totalCash': 65171001344, 'totalCashPerShare': 4.311, 'ebitda': 134660997120, 'totalDebt': 119058997248, 'quickRatio': 0.745, 'currentRatio': 0.867, 'totalRevenue': 391034994688, 'debtToEquity': 209.059, 'revenuePerShare': 25.485, 'returnOnAssets': 0.21464, 'returnOnEquity': 1.5741299, 'freeCashflow': 110846001152, 'operatingCashflow': 118254002176, 'earningsGrowth': -0.341, 'revenueGrowth': 0.061, 'grossMargins': 0.46206, 'ebitdaMargins': 0.34437, 'operatingMargins': 0.31171, 'financialCurrency': 'USD', 'trailingPegRatio': 2.3608}\n</pre> Out[6]: Open High Low Close Volume Dividends Stock Splits Date 2024-11-25 00:00:00-05:00 231.460007 233.250000 229.740005 232.869995 90152800 0.0 0.0 2024-11-26 00:00:00-05:00 233.330002 235.570007 233.330002 235.059998 45986200 0.0 0.0 2024-11-27 00:00:00-05:00 234.470001 235.690002 233.809998 234.929993 33498400 0.0 0.0 2024-11-29 00:00:00-05:00 234.809998 237.809998 233.970001 237.330002 28481400 0.0 0.0 2024-12-02 00:00:00-05:00 237.270004 240.789993 237.160004 239.589996 48137100 0.0 0.0 2024-12-03 00:00:00-05:00 239.809998 242.759995 238.899994 242.649994 38861000 0.0 0.0 2024-12-04 00:00:00-05:00 242.869995 244.110001 241.250000 243.009995 44383900 0.0 0.0 2024-12-05 00:00:00-05:00 243.990005 244.539993 242.130005 243.039993 40033900 0.0 0.0 2024-12-06 00:00:00-05:00 242.910004 244.630005 242.080002 242.839996 36870600 0.0 0.0 2024-12-09 00:00:00-05:00 241.830002 247.240005 241.750000 246.750000 44649200 0.0 0.0 2024-12-10 00:00:00-05:00 246.889999 248.210007 245.339996 247.770004 36914800 0.0 0.0 2024-12-11 00:00:00-05:00 247.960007 250.800003 246.259995 246.490005 45205800 0.0 0.0 2024-12-12 00:00:00-05:00 246.889999 248.740005 245.679993 247.960007 32777500 0.0 0.0 2024-12-13 00:00:00-05:00 247.820007 249.289993 246.240005 248.130005 33155300 0.0 0.0 2024-12-16 00:00:00-05:00 247.990005 251.380005 247.649994 251.039993 51694800 0.0 0.0 2024-12-17 00:00:00-05:00 250.080002 253.830002 249.779999 253.479996 51356400 0.0 0.0 2024-12-18 00:00:00-05:00 252.160004 254.279999 247.740005 248.050003 56774100 0.0 0.0 2024-12-19 00:00:00-05:00 247.500000 252.000000 247.089996 249.789993 60882300 0.0 0.0 2024-12-20 00:00:00-05:00 248.039993 255.000000 245.690002 254.490005 147495300 0.0 0.0 2024-12-23 00:00:00-05:00 254.770004 255.649994 253.449997 255.270004 40858800 0.0 0.0 2024-12-24 00:00:00-05:00 255.490005 258.209991 255.289993 258.200012 23234700 0.0 0.0 In\u00a0[12]: Copied! <pre># Fetch data for multiple stocks\ndata = yf.download([\"AAPL\", \"GOOGL\", \"MSFT\"], start=\"2023-01-01\", end=\"2023-12-31\")\ndata.head()\n</pre> # Fetch data for multiple stocks data = yf.download([\"AAPL\", \"GOOGL\", \"MSFT\"], start=\"2023-01-01\", end=\"2023-12-31\") data.head()  <pre>[*********************100%***********************]  3 of 3 completed\n</pre> Out[12]: Price Close High Low Open Volume Ticker AAPL GOOGL MSFT AAPL GOOGL MSFT AAPL GOOGL MSFT AAPL GOOGL MSFT AAPL GOOGL MSFT Date 2023-01-03 123.768456 88.798103 236.183517 129.537780 90.721133 242.266045 122.877820 88.200264 234.034415 128.924237 89.266399 239.633899 112117500 28131200 25740000 2023-01-04 125.045036 87.761856 225.852081 127.321104 90.322573 229.568624 123.778358 86.954777 222.756597 125.569520 90.023653 228.986992 89113600 34854800 50623400 2023-01-05 123.718979 85.888649 219.158356 126.440361 87.253703 224.324075 123.461690 85.589737 218.616150 125.807022 87.154066 223.979031 80962700 27194400 39585600 2023-01-06 128.271103 87.024529 221.741226 128.934129 87.373270 222.559461 123.590330 84.553490 216.240345 124.698677 86.476520 219.838594 87754700 41381500 43613600 2023-01-09 128.795578 87.702065 223.900162 132.021662 89.724738 227.961764 128.538289 87.542646 223.200236 129.112255 88.040840 223.239662 70790800 29003900 27369800 In\u00a0[10]: Copied! <pre># Get the latest market price\nlatest_price = ticker.history(period=\"1d\")['Close'][-1]\nprint(f\"Latest Close Price: {latest_price}\")\n</pre> # Get the latest market price latest_price = ticker.history(period=\"1d\")['Close'][-1] print(f\"Latest Close Price: {latest_price}\")  <pre>Latest Close Price: 258.20001220703125\n</pre> <pre>/var/folders/kx/c6xk17ln6blbs7f3nf_p1str0000gn/T/ipykernel_7776/404107778.py:2: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n  latest_price = ticker.history(period=\"1d\")['Close'][-1]\n</pre> In\u00a0[11]: Copied! <pre># Fetch historical data for a specific stock\ndata = yf.download(\"AAPL\", start=\"2023-01-01\", end=\"2023-12-31\")\n\n# Save to CSV\ndata.to_csv(\"AAPL_data.csv\")\n\nprint(\"Data saved to AAPL_data.csv\")\n</pre> # Fetch historical data for a specific stock data = yf.download(\"AAPL\", start=\"2023-01-01\", end=\"2023-12-31\")  # Save to CSV data.to_csv(\"AAPL_data.csv\")  print(\"Data saved to AAPL_data.csv\") <pre>[*********************100%***********************]  1 of 1 completed</pre> <pre>Data saved to AAPL_data.csv\n</pre> <pre>\n</pre>"},{"location":"python_tutorial/02_Data_Gathering/#data","title":"Data \ubc1b\uae30\u00b6","text":"<p>Python Library \ub97c \uc0ac\uc6a9\ud574\uc11c Data \ub97c \ubc1b\uc544\uc624\uaca0\uc2b5\ub2c8\ub2e4.</p> <p>yfinance \ub77c\uc774\ube0c\ub7ec\ub9ac\ub85c \uc2e4\uc2dc\uac04 \uc8fc\uc2dd \ub370\uc774\ud130\ub97c \ubc1b\ub294 \uc2e4\uc2b5\uc744 \ud574\ubd05\ub2c8\ub2e4.</p>"},{"location":"python_tutorial/02_Data_Gathering/","title":"\ubaa9\ud45c\u00b6","text":"<ul> <li>\uc2e4\uc2dc\uac04 Data\ub97c \ud655\ubcf4\ud558\ub294 \uacbd\ud5d8\uc744 \ud574\ubd05\ub2c8\ub2e4.</li> <li>\ud30c\uc774\uc36c \ub77c\uc774\ube0c\ub7ec\ub9ac\ub97c \uc0ac\uc6a9\ud574\ubd05\ub2c8\ub2e4.</li> </ul>"},{"location":"python_tutorial/02_Data_Gathering/#1-python","title":"1. Python \ub77c\uc774\ube0c\ub7ec\ub9ac\u00b6","text":"<ul> <li>pip : Python \ud328\ud0a4\uc9c0 \ub9e4\ub2c8\uc800. \uc571\uc2a4\ud1a0\uc5b4\uc640 \ube44\uc2b7\ud55c \uc5ed\ud560\uc785\ub2c8\ub2e4.</li> </ul> <p><code>pip install LIBRARY_NAME</code></p> <p>\uba85\ub839\uc5b4\ub97c \ud1b5\ud574 \uc124\uce58\uac00 \uac00\ub2a5\ud569\ub2c8\ub2e4.</p> <ul> <li>yfinance : https://github.com/ranaroussi/yfinance?tab=readme-ov-file</li> <li>Yahoo Finance API wrapper \ub77c\uc774\ube0c\ub7ec\ub9ac\ub97c \uc608\uc2dc\ub85c \ud65c\uc6a9\ud574\ubcf4\uaca0\uc2b5\ub2c8\ub2e4.</li> </ul> <p><code> pip install -U yfinance</code></p>"},{"location":"python_tutorial/03_Pandas/","title":"Pandas","text":"In\u00a0[\u00a0]: Copied! <pre># print working directory\n\n%pwd\n</pre> # print working directory  %pwd In\u00a0[\u00a0]: Copied! <pre># pandas \uc124\uce58\n\n%pip install -U pandas\n</pre> # pandas \uc124\uce58  %pip install -U pandas In\u00a0[12]: Copied! <pre>import pandas as pd\n\ndf = pd.read_csv(\"./AAPL_data.csv\")\ndf\n</pre> import pandas as pd  df = pd.read_csv(\"./AAPL_data.csv\") df Out[12]: Price Close High Low Open Volume 0 Ticker AAPL AAPL AAPL AAPL AAPL 1 Date NaN NaN NaN NaN NaN 2 2023-01-03 123.7684555053711 129.53777972145332 122.87781986866916 128.92423659950236 112117500 3 2023-01-04 125.04503631591797 127.32110440506466 123.77835783326215 125.56951966733021 89113600 4 2023-01-05 123.71897888183594 126.44036106917035 123.46169000194207 125.80702181866339 80962700 ... ... ... ... ... ... ... 247 2023-12-22 192.6561737060547 194.45734722393942 192.02924020237552 194.22845757962483 37122800 248 2023-12-26 192.10885620117188 192.94475743470915 191.88992751842636 192.66612369019674 28919300 249 2023-12-27 192.20835876464844 192.5566585360189 190.15840400250286 191.55158790358445 48087700 250 2023-12-28 192.6362762451172 193.71101293849657 192.22827139973785 193.1935437488545 34049900 251 2023-12-29 191.5913848876953 193.452263485871 190.7952819760843 192.9547010641641 42628800 <p>252 rows \u00d7 6 columns</p> In\u00a0[13]: Copied! <pre>df = df[2:]\ndf\n</pre> df = df[2:] df Out[13]: Price Close High Low Open Volume 2 2023-01-03 123.7684555053711 129.53777972145332 122.87781986866916 128.92423659950236 112117500 3 2023-01-04 125.04503631591797 127.32110440506466 123.77835783326215 125.56951966733021 89113600 4 2023-01-05 123.71897888183594 126.44036106917035 123.46169000194207 125.80702181866339 80962700 5 2023-01-06 128.27110290527344 128.93412872931725 123.59032994150238 124.6986773645302 87754700 6 2023-01-09 128.7955780029297 132.02166222689684 128.53828914886986 129.11225514638548 70790800 ... ... ... ... ... ... ... 247 2023-12-22 192.6561737060547 194.45734722393942 192.02924020237552 194.22845757962483 37122800 248 2023-12-26 192.10885620117188 192.94475743470915 191.88992751842636 192.66612369019674 28919300 249 2023-12-27 192.20835876464844 192.5566585360189 190.15840400250286 191.55158790358445 48087700 250 2023-12-28 192.6362762451172 193.71101293849657 192.22827139973785 193.1935437488545 34049900 251 2023-12-29 191.5913848876953 193.452263485871 190.7952819760843 192.9547010641641 42628800 <p>250 rows \u00d7 6 columns</p> In\u00a0[\u00a0]: Copied! <pre>df.head()\n</pre> df.head() In\u00a0[\u00a0]: Copied! <pre>df.tail(n=10)\n</pre> df.tail(n=10) In\u00a0[\u00a0]: Copied! <pre>df.shape\n</pre> df.shape In\u00a0[\u00a0]: Copied! <pre>df.index\n</pre> df.index In\u00a0[\u00a0]: Copied! <pre>df.columns\n</pre> df.columns In\u00a0[14]: Copied! <pre>df.dtypes\n</pre> df.dtypes Out[14]: <pre>Price     object\nClose     object\nHigh      object\nLow       object\nOpen      object\nVolume    object\ndtype: object</pre> In\u00a0[\u00a0]: Copied! <pre>df.sample(n=10)\ndf.sample(frac=0.1)\n</pre> df.sample(n=10) df.sample(frac=0.1) In\u00a0[24]: Copied! <pre>df.loc[:, \"Volume\"] = df[\"Volume\"].astype('float')\n</pre> df.loc[:, \"Volume\"] = df[\"Volume\"].astype('float') In\u00a0[19]: Copied! <pre>df.nlargest(10,\"Volume\")\n</pre> df.nlargest(10,\"Volume\") Out[19]: Price Close High Low Open Volume 24 2023-02-03 152.89219665527344 155.74223078414798 146.29160978319123 146.4895254643634 154357300.0 242 2023-12-15 196.6068115234375 197.43275173465906 196.0395831061416 196.5669980286803 128256700.0 107 2023-06-05 178.2287139892578 183.55830143833987 176.70029356551757 181.25576664565278 121946500.0 23 2023-02-02 149.25050354003906 149.60674271507267 146.62807162382762 147.35047067319957 118339000.0 149 2023-08-04 180.62060546875 185.97004732736383 180.5511249208849 184.12404245746146 115799700.0 87 2023-05-05 172.0260009765625 172.74950296676494 169.24098486047572 169.45902904213452 113316400.0 172 2023-09-07 176.46188354492188 177.10787273977496 172.46674085776903 174.0965977246818 112488800.0 2 2023-01-03 123.7684555053711 129.53777972145332 122.87781986866916 128.92423659950236 112117500.0 178 2023-09-15 173.92764282226562 175.40843335624928 172.74501513151833 175.38855280048267 109205100.0 116 2023-06-16 183.52854919433594 185.58298054193108 182.88344624177174 185.32492724572717 101235600.0 In\u00a0[20]: Copied! <pre>df.sort_values('Volume')\n</pre> df.sort_values('Volume') Out[20]: Price Close High Low Open Volume 227 2023-11-24 189.0438690185547 189.96932784078396 188.3273779116158 189.93947531002817 24048300.0 248 2023-12-26 192.10885620117188 192.94475743470915 191.88992751842636 192.66612369019674 28919300.0 126 2023-07-03 191.0117950439453 192.4211080946957 190.3170502473471 192.32185451119005 31458200.0 250 2023-12-28 192.6362762451172 193.71101293849657 192.22827139973785 193.1935437488545 34049900.0 146 2023-08-01 194.1381072998047 195.24967486566203 193.81058861115622 194.7633716275856 35175100.0 ... ... ... ... ... ... ... 149 2023-08-04 180.62060546875 185.97004732736383 180.5511249208849 184.12404245746146 115799700.0 23 2023-02-02 149.25050354003906 149.60674271507267 146.62807162382762 147.35047067319957 118339000.0 107 2023-06-05 178.2287139892578 183.55830143833987 176.70029356551757 181.25576664565278 121946500.0 242 2023-12-15 196.6068115234375 197.43275173465906 196.0395831061416 196.5669980286803 128256700.0 24 2023-02-03 152.89219665527344 155.74223078414798 146.29160978319123 146.4895254643634 154357300.0 <p>250 rows \u00d7 6 columns</p> In\u00a0[7]: Copied! <pre>df[\"Close\"]\n</pre> df[\"Close\"] Out[7]: <pre>0                    AAPL\n1                     NaN\n2       123.7684555053711\n3      125.04503631591797\n4      123.71897888183594\n              ...        \n247     192.6561737060547\n248    192.10885620117188\n249    192.20835876464844\n250     192.6362762451172\n251     191.5913848876953\nName: Close, Length: 252, dtype: object</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[25]: Copied! <pre>df.loc[:,\"Close\"] = df[\"Close\"].astype('float')\ndf.loc[:,\"Open\"] = df['Open'].astype('float')\ndf['Close'] - df['Open']\n</pre> df.loc[:,\"Close\"] = df[\"Close\"].astype('float') df.loc[:,\"Open\"] = df['Open'].astype('float') df['Close'] - df['Open'] Out[25]: <pre>2     -5.155781\n3     -0.524483\n4     -2.088043\n5      3.572426\n6     -0.316677\n         ...   \n247   -1.572284\n248   -0.557267\n249    0.656771\n250   -0.557268\n251   -1.363316\nLength: 250, dtype: float64</pre> In\u00a0[26]: Copied! <pre>df[\"Close\"] &gt; df['Open']\n</pre> df[\"Close\"] &gt; df['Open'] Out[26]: <pre>2      False\n3      False\n4      False\n5       True\n6      False\n       ...  \n247    False\n248    False\n249     True\n250    False\n251    False\nLength: 250, dtype: bool</pre> In\u00a0[27]: Copied! <pre>df[df[\"Close\"] &gt; df['Open']]\n</pre> df[df[\"Close\"] &gt; df['Open']] Out[27]: Price Close High Low Open Volume 5 2023-01-06 128.271103 128.93412872931725 123.59032994150238 124.698677 87754700.0 7 2023-01-10 129.369583 129.89406659487034 126.78674290984911 128.904473 63896200.0 8 2023-01-11 132.100845 132.12062633545213 129.1023781585197 129.884150 69458900.0 10 2023-01-13 133.357620 133.51595882992456 130.28988932008468 130.656034 57809700.0 11 2023-01-17 134.525345 135.86128703391867 132.73418300243557 133.426895 63646600.0 ... ... ... ... ... ... ... 240 2023-12-13 196.994919 197.03471713546693 193.90007998207173 194.138900 70404200.0 241 2023-12-14 197.144165 198.6467979467734 195.20367481138322 197.054607 66831600.0 242 2023-12-15 196.606812 197.43275173465906 196.0395831061416 196.566998 128256700.0 244 2023-12-19 195.979889 195.98983469805682 194.9350047944863 195.203693 40714100.0 249 2023-12-27 192.208359 192.5566585360189 190.15840400250286 191.551588 48087700.0 <p>151 rows \u00d7 6 columns</p> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[28]: Copied! <pre>df.describe()\n</pre> df.describe() Out[28]: Close Open Volume count 250.000000 250.000000 2.500000e+02 mean 171.281995 170.992146 5.921703e+07 std 17.418788 17.615621 1.777392e+07 min 123.718979 124.698677 2.404830e+07 25% 160.670422 160.117881 4.781208e+07 50% 174.389793 174.161200 5.507750e+07 75% 186.265335 185.399351 6.574292e+07 max 197.144165 197.054607 1.543573e+08 In\u00a0[31]: Copied! <pre>df[[\"Close\", \"Open\", \"Volume\"]].corr()\n</pre> df[[\"Close\", \"Open\", \"Volume\"]].corr() Out[31]: Close Open Volume Close 1.000000 0.994800 -0.321075 Open 0.994800 1.000000 -0.324514 Volume -0.321075 -0.324514 1.000000 In\u00a0[34]: Copied! <pre>df[\"date\"] = pd.to_datetime(df['Price'])\ndf['weekday'] = df['date'].dt.weekday\ndf\n</pre> df[\"date\"] = pd.to_datetime(df['Price']) df['weekday'] = df['date'].dt.weekday df <pre>/var/folders/kx/c6xk17ln6blbs7f3nf_p1str0000gn/T/ipykernel_8397/3916596524.py:1: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df[\"date\"] = pd.to_datetime(df['Price'])\n/var/folders/kx/c6xk17ln6blbs7f3nf_p1str0000gn/T/ipykernel_8397/3916596524.py:2: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df['weekday'] = df['date'].dt.weekday\n</pre> Out[34]: Price Close High Low Open Volume date weekday 2 2023-01-03 123.768456 129.53777972145332 122.87781986866916 128.924237 112117500.0 2023-01-03 1 3 2023-01-04 125.045036 127.32110440506466 123.77835783326215 125.569520 89113600.0 2023-01-04 2 4 2023-01-05 123.718979 126.44036106917035 123.46169000194207 125.807022 80962700.0 2023-01-05 3 5 2023-01-06 128.271103 128.93412872931725 123.59032994150238 124.698677 87754700.0 2023-01-06 4 6 2023-01-09 128.795578 132.02166222689684 128.53828914886986 129.112255 70790800.0 2023-01-09 0 ... ... ... ... ... ... ... ... ... 247 2023-12-22 192.656174 194.45734722393942 192.02924020237552 194.228458 37122800.0 2023-12-22 4 248 2023-12-26 192.108856 192.94475743470915 191.88992751842636 192.666124 28919300.0 2023-12-26 1 249 2023-12-27 192.208359 192.5566585360189 190.15840400250286 191.551588 48087700.0 2023-12-27 2 250 2023-12-28 192.636276 193.71101293849657 192.22827139973785 193.193544 34049900.0 2023-12-28 3 251 2023-12-29 191.591385 193.452263485871 190.7952819760843 192.954701 42628800.0 2023-12-29 4 <p>250 rows \u00d7 8 columns</p> In\u00a0[38]: Copied! <pre>df.groupby(\"weekday\")[['Open', 'Close', 'Volume']].mean()\n</pre> df.groupby(\"weekday\")[['Open', 'Close', 'Volume']].mean() Out[38]: Open Close Volume weekday 0 171.471788 171.955209 5.626837e+07 1 170.407503 170.581198 5.477188e+07 2 170.997955 170.949820 5.886871e+07 3 170.648236 170.965807 6.017473e+07 4 171.491563 172.043658 6.566139e+07"},{"location":"python_tutorial/03_Pandas/#pandas","title":"Pandas\u00b6","text":"<p>Pandas \ub294 \uc5d1\uc140\uacfc \ube44\uc2b7\ud55c \uc5ed\ud560\uc744 \ud558\ub294 Python\uc758 \uc8fc\uc694\ud55c \ub77c\uc774\ube0c\ub7ec\ub9ac \uc785\ub2c8\ub2e4. \ub370\uc774\ud130\ub97c \uc77d\uace0, \ucc98\ub9ac\ud558\uace0, \ubd84\uc11d\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.</p> <ul> <li>Pandas \ub77c\uc774\ube0c\ub7ec\ub9ac\ub97c \uc774\uc6a9\ud574\uc11c \uc5d1\uc140 \ud615\uc2dd\uc758 \ud30c\uc77c (Spreadsheet, ex. csv, xlsx, ...) \uc744 \uc5f4\uc5b4\ubd05\ub2c8\ub2e4.</li> <li>DataFrame\ub97c \uc0ac\uc6a9\ud558\ub294 \ubc95\uc744 \ubc30\uc6cc\ubd05\ub2c8\ub2e4.</li> </ul>"},{"location":"python_tutorial/03_Pandas/#1","title":"1. \ud30c\uc77c \ub85c\ub4dc\ud558\uace0 \uc0b4\ud3b4\ubcf4\uae30\u00b6","text":"<ul> <li>\uacbd\ub85c (Path) = \ud30c\uc77c \uc704\uce58 &amp; \ud30c\uc77c \uc774\ub984</li> </ul> <ul> <li><code>/</code> : root. Windows\uc5d0\uc11c\ub294 \uae30\ubcf8\uc635\uc158\uc5d0\uc11c C:\\</li> <li><code>~/</code> : \uc0ac\uc6a9\uc790 \ud3f4\ub354. Windows\uc5d0\uc11c\ub294 C:\\Users\\\uc0ac\uc6a9\uc790\uacc4\uc815\uc774\ub984</li> <li><code>./</code> : \ud604\uc7ac \uc791\uc5c5 \ud3f4\ub354(working directory), \ubcc4\ub3c4\ub85c \uc791\uc5c5\ud558\uc9c0 \uc54a\uc740 \uacbd\uc6b0 \uc0dd\ub7b5 \uac00\ub2a5</li> <li><code>../</code> : \ud604\uc7ac \ud3f4\ub354\uc758 \uc0c1\uc704 \ud3f4\ub354</li> </ul> <ul> <li>Windows \uc5d0\ub294 \"\ud55c\uae00\" \uc744 \uc870\uc2ec\ud560 \uac83</li> </ul>"},{"location":"python_tutorial/03_Pandas/#2","title":"2. \ub370\uc774\ud130 \ub2e4\ub8e8\uae30\u00b6","text":"<ul> <li>\ub370\uc774\ud130 \ud0c0\uc785 \uc124\uc815\ud558\uae30</li> <li>\ub370\uc774\ud130 \uc120\ud0dd\ud558\uae30, \uc815\ub82c\ud558\uae30</li> <li>\ub370\uc774\ud130 \uc120\ud0dd\uc744 \uc704\ud55c \uc870\uac74 \uc0dd\uc131\ud558\uace0 \uadf8\ub300\ub85c \uc0ac\uc6a9\ud558\uae30</li> </ul> <p>\uc560\ud50c\uc740 1\ub144\ub3d9\uc548 \uba70\uce60\uc774\ub098 \uc0c1\uc2b9\uc744 \ud588\uace0, \uba70\uce60\uc774\ub098 \ud558\ub77d\uc744 \ud588\uc744\uae4c\uc694?</p>"},{"location":"python_tutorial/03_Pandas/#3","title":"3. \ub370\uc774\ud130 \uc9d1\uacc4\ud558\uae30\u00b6","text":"<ul> <li>\ud3c9\uade0\ub0b4\uace0, \ud3b8\ucc28 \uad6c\ud558\uace0, \ub2e4\uc591\ud55c \uacc4\uc0b0 \ud589\ud558\uae30</li> </ul>"},{"location":"python_tutorial/04_Visualization/","title":"\uc2dc\uac01\ud654\ud558\uae30 (Visualization)","text":"In\u00a0[1]: Copied! <pre># \ud544\uc694 \ub77c\uc774\ube0c\ub7ec\ub9ac \uc124\uce58\n%pip install -U matplotlib seaborn plotly\n</pre> # \ud544\uc694 \ub77c\uc774\ube0c\ub7ec\ub9ac \uc124\uce58 %pip install -U matplotlib seaborn plotly  <pre>Successfully installed contourpy-1.3.1 cycler-0.12.1 fonttools-4.55.3 kiwisolver-1.4.8 matplotlib-3.10.0 pillow-11.0.0 plotly-5.24.1 pyparsing-3.2.0 seaborn-0.13.2 tenacity-9.0.0\n\n[notice] A new release of pip is available: 24.0 -&gt; 24.3.1\n[notice] To update, run: pip install --upgrade pip\nNote: you may need to restart the kernel to use updated packages.\n</pre> In\u00a0[1]: Copied! <pre>import pandas as pd\n\ndf = pd.read_csv(\"./AAPL_data.csv\")[2:]\ndf\n</pre> import pandas as pd  df = pd.read_csv(\"./AAPL_data.csv\")[2:] df Out[1]: Price Close High Low Open Volume 2 2023-01-03 123.7684555053711 129.53777972145332 122.87781986866916 128.92423659950236 112117500 3 2023-01-04 125.04503631591797 127.32110440506466 123.77835783326215 125.56951966733021 89113600 4 2023-01-05 123.71897888183594 126.44036106917035 123.46169000194207 125.80702181866339 80962700 5 2023-01-06 128.27110290527344 128.93412872931725 123.59032994150238 124.6986773645302 87754700 6 2023-01-09 128.7955780029297 132.02166222689684 128.53828914886986 129.11225514638548 70790800 ... ... ... ... ... ... ... 247 2023-12-22 192.6561737060547 194.45734722393942 192.02924020237552 194.22845757962483 37122800 248 2023-12-26 192.10885620117188 192.94475743470915 191.88992751842636 192.66612369019674 28919300 249 2023-12-27 192.20835876464844 192.5566585360189 190.15840400250286 191.55158790358445 48087700 250 2023-12-28 192.6362762451172 193.71101293849657 192.22827139973785 193.1935437488545 34049900 251 2023-12-29 191.5913848876953 193.452263485871 190.7952819760843 192.9547010641641 42628800 <p>250 rows \u00d7 6 columns</p> In\u00a0[17]: Copied! <pre>df[\"Open\"] = df[\"Open\"].astype('float')\ndf[\"Volume\"] = df[\"Volume\"].astype('float')\ndf[\"Close\"] = df[\"Close\"].astype('float')\ndf['date'] = pd.to_datetime(df[\"Price\"])\ndf.dtypes\n</pre> df[\"Open\"] = df[\"Open\"].astype('float') df[\"Volume\"] = df[\"Volume\"].astype('float') df[\"Close\"] = df[\"Close\"].astype('float') df['date'] = pd.to_datetime(df[\"Price\"]) df.dtypes   Out[17]: <pre>Price                   object\nClose                  float64\nHigh                    object\nLow                     object\nOpen                   float64\nVolume                 float64\ndate            datetime64[ns]\ndate_numeric             int64\ndtype: object</pre> In\u00a0[3]: Copied! <pre>import seaborn as sns\n\nsns.lineplot(df, x = \"date\", y = 'Close')\n</pre> import seaborn as sns  sns.lineplot(df, x = \"date\", y = 'Close') Out[3]: <pre>&lt;Axes: xlabel='date', ylabel='Close'&gt;</pre> In\u00a0[18]: Copied! <pre>df['date_numeric'] = (df['date'] - pd.Timestamp(\"2023-01-01\")) // pd.Timedelta('1d')\nsns.regplot(x='date_numeric', y='Close', data=df)\n</pre> df['date_numeric'] = (df['date'] - pd.Timestamp(\"2023-01-01\")) // pd.Timedelta('1d') sns.regplot(x='date_numeric', y='Close', data=df) Out[18]: <pre>&lt;Axes: xlabel='date_numeric', ylabel='Close'&gt;</pre> In\u00a0[4]: Copied! <pre>import plotly.express as px\n\npx.line(df, x = \"date\", y = 'Close')\n</pre> import plotly.express as px  px.line(df, x = \"date\", y = 'Close') In\u00a0[5]: Copied! <pre>import plotly.graph_objects as go\n\n# Create a figure\nfig = go.Figure()\n\n# Add line plot for 'value'\nfig.add_trace(go.Scatter(x=df['date'], y=df['Close'], name='Close', mode='lines', yaxis='y1'))\n\n# Add bar plot for 'volume' on secondary y-axis\nfig.add_trace(go.Bar(x=df['date'], y=df['Volume'], name='Volume', yaxis='y2', opacity=0.6))\n\n# Update layout for dual axes\nfig.update_layout(\n    title='Time vs Value and Volume',\n    xaxis=dict(title='Time'),\n    yaxis=dict(\n        title='Value',\n        titlefont=dict(color='blue'),\n        tickfont=dict(color='blue'),\n    ),\n    yaxis2=dict(\n        title='Volume',\n        titlefont=dict(color='orange'),\n        tickfont=dict(color='orange'),\n        anchor='x',\n        overlaying='y',\n        side='right',\n    ),\n    legend=dict(orientation=\"h\", yanchor=\"bottom\", y=1.02, xanchor=\"right\", x=1),\n    xaxis_tickformat='%Y-%m-%d %H:%M',\n)\n\n# Show the figure\nfig.show()\n</pre> import plotly.graph_objects as go  # Create a figure fig = go.Figure()  # Add line plot for 'value' fig.add_trace(go.Scatter(x=df['date'], y=df['Close'], name='Close', mode='lines', yaxis='y1'))  # Add bar plot for 'volume' on secondary y-axis fig.add_trace(go.Bar(x=df['date'], y=df['Volume'], name='Volume', yaxis='y2', opacity=0.6))  # Update layout for dual axes fig.update_layout(     title='Time vs Value and Volume',     xaxis=dict(title='Time'),     yaxis=dict(         title='Value',         titlefont=dict(color='blue'),         tickfont=dict(color='blue'),     ),     yaxis2=dict(         title='Volume',         titlefont=dict(color='orange'),         tickfont=dict(color='orange'),         anchor='x',         overlaying='y',         side='right',     ),     legend=dict(orientation=\"h\", yanchor=\"bottom\", y=1.02, xanchor=\"right\", x=1),     xaxis_tickformat='%Y-%m-%d %H:%M', )  # Show the figure fig.show() In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"python_tutorial/04_Visualization/#visualization","title":"\uc2dc\uac01\ud654\ud558\uae30 (Visualization)\u00b6","text":"<p>pandas Dataframe \uc5d0 \ub2f4\uae34 \ub370\uc774\ud130\ub97c \uadf8\ub798\ud504\ub85c \uadf8\ub824 \ubd05\ub2c8\ub2e4.</p> <ul> <li>Seaborn \ub77c\uc774\ube0c\ub7ec\ub9ac\ub85c \uc774\ubbf8\uc9c0 \ud615\ud0dc\ub85c \uadf8\ub824\ubd05\ub2c8\ub2e4.</li> <li>plotly \ub77c\uc774\ube0c\ub7ec\ub9ac\ub85c \ubc18\uc751\ud615 \uadf8\ub798\ud504\ub97c \uadf8\ub824\ubd05\ub2c8\ub2e4.</li> </ul>"}]}